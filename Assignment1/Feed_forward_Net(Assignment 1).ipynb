{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     1,
     19,
     23,
     27,
     33,
     37,
     41,
     45,
     49,
     53,
     58,
     80,
     93,
     100,
     140,
     177
    ]
   },
   "outputs": [],
   "source": [
    "class Neural_Network():\n",
    "    def __init__(self, neurons, Activations, epochs=1000, learning_rate=0.001, loss='squared'): \n",
    "        # arguments: an array \"neurons\" consist of number of neurons for each layer, \n",
    "        # an array \"activations\" consisting of activation functions used for the hidden layers and output layer\n",
    "        self.inputSize = neurons[0] # Number of neurons in input layer\n",
    "        self.outputSize = neurons[-1] # Number of neurons in output layer\n",
    "        self.layers = len(neurons)\n",
    "        self.weights = [] #weights for each layer\n",
    "        self.biases = [] #biases in each layer \n",
    "        self.layer_activations = [] #activations in each layer\n",
    "        self.epochs = epochs #number of epochs to train the network\n",
    "        self.learning_rate = learning_rate #learning rate used for training\n",
    "        self.loss_function = loss\n",
    "        np.random.seed(0)\n",
    "        for i in range(len(neurons)-1): \n",
    "            self.weights.append(np.random.rand(neurons[i+1],neurons[i])) #weight matrix between layer i and layer i+1\n",
    "            self.biases.append(np.random.rand(neurons[i+1],1))\n",
    "            self.layer_activations.append(Activations[i]) #activations for each layer\n",
    "        \n",
    "    def sigmoid(self, z): # sigmoid activation function\n",
    "        #Fill in the details to compute and return the sigmoid activation function                  \n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    def sigmoidPrime(self, z): # derivative of sigmoid activation function\n",
    "        #Fill in the details to compute and return the derivative of sigmoid activation function\n",
    "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
    "                          \n",
    "    def tanh(self, z): # hyperbolic tan activation function\n",
    "        #Fill in the details to compute and return the tanh activation function                  \n",
    "        exp_z = np.exp(z)\n",
    "        exp_z_negative = np.exp(-z)\n",
    "        return (exp_z - exp_z_negative) / (exp_z + exp_z_negative)  \n",
    "    \n",
    "    def tanhPrime(self, z): # derivative of hyperbolic tan activation function\n",
    "        #Fill in the details to compute and return the derivative of tanh activation function\n",
    "        return 1 - self.tanh(z) ** 2\n",
    "                          \n",
    "    def linear(self, z): # Linear activation function\n",
    "        #Fill in the details to compute and return the linear activation function                                    \n",
    "        return z\n",
    "    \n",
    "    def linearPrime(self, z): # derivative of linear activation function\n",
    "        #Fill in the details to compute and return the derivative of activation function                                                      \n",
    "        return np.ones(z.shape)\n",
    "\n",
    "    def ReLU(self, z): # ReLU activation function\n",
    "        #Fill in the details to compute and return the ReLU activation function                  \n",
    "        return np.where(z < 0, 0, z)\n",
    "    \n",
    "    def ReLUPrime(self, z): # derivative of ReLU activation function\n",
    "        #Fill in the details to compute and return the derivative of ReLU activation function\n",
    "        return np.where(z < 0, 0, 1)\n",
    "    \n",
    "    def softmax(self, z): # Softmax activation function\n",
    "        z = z - np.max(z, axis=0)\n",
    "        z_exp = np.exp(z)\n",
    "        return z_exp / np.sum(z_exp, axis=0)          \n",
    "    \n",
    "    def forward(self, a): # function of forward pass which will receive input and give the output of final layer\n",
    "        # Write the forward pass using the weights and biases to find the predicted value and return them.\n",
    "        layer_activations_a = [a] #store the input as the input layer activations\n",
    "        layer_dot_prod_z = []\n",
    "        for i, param in enumerate(zip(self.biases, self.weights)):\n",
    "            b, w = param[0], param[1]\n",
    "            z = np.dot(w, a) + b\n",
    "            if self.layer_activations[i].lower()  == 'sigmoid':\n",
    "                a = self.sigmoid(z)\n",
    "            elif self.layer_activations[i].lower() == 'relu':\n",
    "                a = self.ReLU(z)    \n",
    "            elif self.layer_activations[i].lower() == 'tanh':   \n",
    "                a = self.tanh(z)\n",
    "            elif self.layer_activations[i].lower() == 'linear':\n",
    "                a = self.linear(z)\n",
    "            elif self.layer_activations[i].lower() == 'softmax':\n",
    "                a = self.softmax(z)\n",
    "            layer_dot_prod_z.append(z)\n",
    "            layer_activations_a.append(a)\n",
    "\n",
    "        return a, layer_dot_prod_z, layer_activations_a\n",
    "        \n",
    "    def loss(self, Y_hat, Y):\n",
    "        #Implement the loss function\n",
    "        epsilon = 10 ** -5\n",
    "#         print(f'Y : {Y.shape}')\n",
    "#         print(f'Y_hat : {Y_hat.shape}')\n",
    "        if self.loss_function.lower() == 'cross-entropy':\n",
    "            Y_hat = np.where(Y_hat == 1, 1 - epsilon, Y_hat)\n",
    "            Y_hat = np.where(Y_hat == 0, epsilon, Y_hat)\n",
    "            error = - Y * np.log(Y_hat) - (1 - Y) * np.log(1 - Y_hat) \n",
    "        elif self.loss_function.lower() == 'squared':\n",
    "            error = 0.5 * (Y_hat - Y) ** 2\n",
    "        return error\n",
    "\n",
    "    def loss_grad(self, Y_hat, Y):\n",
    "        #Return the gradient of the loss function -\n",
    "        # 1. w.r.t. activations for squared-error \n",
    "        # 2. w.r.t. z for cross-entropy loss\n",
    "        grad = Y_hat - Y\n",
    "        return grad\n",
    "        \n",
    "    def backward(self, x, y, zs, activations): # find the loss and return derivative of loss w.r.t every parameter\n",
    "        # Write the backpropagation algorithm here to find the gradients of weights and biases and return them.\n",
    "        # Assuming L2 loss\n",
    "        grad_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        grad_w = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "        delta = self.loss_grad(activations[-1], y)\n",
    "        if self.layer_activations[-1].lower() == 'sigmoid':\n",
    "            delta = delta * self.sigmoidPrime(zs[-1])\n",
    "        elif self.layer_activations[-1].lower() == 'relu':\n",
    "            delta = delta * self.ReLUPrime(zs[-1])\n",
    "        elif self.layer_activations[-1].lower() == 'tanh':\n",
    "            delta = delta * self.tanhPrime(zs[-1])   \n",
    "        elif self.layer_activations[-1].lower() == 'linear':\n",
    "            delta = delta * self.linearPrime(zs[-1])\n",
    "        \n",
    "        # Number of training examples = m\n",
    "        m = delta.shape[1]\n",
    "        # fill in the appropriate details for gradients of w and b\n",
    "        grad_w[-1] = np.dot(delta, activations[-2].T) / m \n",
    "        grad_b[-1] = np.sum(delta, axis=1, keepdims=True) / m\n",
    "\n",
    "        for l in range(2, self.layers): # Here l is in backward sense i.e. last lth layer\n",
    "            z = zs[-l]\n",
    "            if self.layer_activations[-l].lower() == 'sigmoid':\n",
    "                prime = self.sigmoidPrime(z)\n",
    "            elif self.layer_activations[-l].lower() == 'relu':\n",
    "                prime = self.ReLUPrime(z)\n",
    "            elif self.layer_activations[-l].lower() == 'tanh':   \n",
    "                prime = self.tanhPrime(z)\n",
    "            elif self.layer_activations[-l].lower() == 'linear':\n",
    "                prime = self.linearPrime(z)\n",
    "\n",
    "            #Compute delta, gradients of b and w\n",
    "            delta = np.dot(self.weights[-l+1].T, delta) * prime\n",
    "            grad_w[-l] = np.dot(delta, activations[-l-1].T) / m \n",
    "            grad_b[-l] = np.sum(delta, axis=1, keepdims=True) / m\n",
    "\n",
    "        return (grad_b, grad_w)   \n",
    "\n",
    "    def update_parameters(self, grads, learning_rate): # update the parameters using the gradients\n",
    "        # update weights and biases using the gradients and the learning rate\n",
    "        grad_b, grad_w = grads[0], grads[1]       \n",
    "        \n",
    "        #Implement the update rule for weights  and biases\n",
    "        self.weights = [self.weights[i] - learning_rate * grad_w[i] for i in range(len(self.weights))]\n",
    "        self.biases = [self.biases[i] - learning_rate * grad_b[i] for i in range(len(self.biases))] \n",
    "        \n",
    "    def train(self, X, Y, minibatch=False, batch_size=20, verbose=False): # receive the full training data set\n",
    "        lr = self.learning_rate # learning rate\n",
    "        epochs = self.epochs # number of epochs\n",
    "        loss_list = []\n",
    "        for e in range(epochs): \n",
    "            losses = []\n",
    "            for q in range(len(X)):\n",
    "                if minibatch == False:\n",
    "                    rows_x, cols_x = X[q].shape[0], 1\n",
    "                    rows_y, cols_y = Y[q].shape[0], 1\n",
    "                else:\n",
    "                    rows_x, cols_x = X[q].shape[1], X[q].shape[0]\n",
    "                    rows_y, cols_y = Y[q].shape[1], Y[q].shape[0]\n",
    "\n",
    "                train_x = np.resize(X[q], (rows_x, cols_x))\n",
    "                train_y = np.resize(Y[q],(rows_y, cols_y))\n",
    "                \n",
    "                out, dot_prod_z, activations_a = self.forward(train_x)\n",
    "                loss = self.loss(out, train_y)\n",
    "                grads = self.backward(train_x, train_y, dot_prod_z, activations_a) # find the gradients using backward pass\n",
    "                self.update_parameters(grads, lr)\n",
    "                losses.append(loss)\n",
    "            \n",
    "            loss_mean = np.mean(np.array(losses))\n",
    "            loss_list.append(loss_mean)\n",
    "            if verbose:\n",
    "                print(f'Epoch: {e} Loss: {loss_mean}')\n",
    "        return loss_list\n",
    "        \n",
    "    def predict(self, x):\n",
    "        print (\"Input : \\n\" + str(x))\n",
    "        prediction,_,_ = self.forward(x)\n",
    "        print (\"Output: \\n\" + str(prediction))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Miscellaneous Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### One-hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# a method for creating one hot encoded labels \n",
    "def onehotencoding(Y):\n",
    "    rows = Y.shape[0]\n",
    "    values = {e:i for i, e in enumerate(np.unique(Y))}\n",
    "    y_enc = np.zeros((rows, len(values)))\n",
    "    for i in range(rows):\n",
    "        j = values[Y[i]]\n",
    "        y_enc[i][j] = 1\n",
    "    return y_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Create mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#a method to create minibatches \n",
    "def create_minibatches(X, Y, minibatchsize):\n",
    "    numbatches = int(np.ceil(len(X)/minibatchsize))\n",
    "    idx = np.arange(len(X))\n",
    "    np.random.shuffle(idx)\n",
    "    X_minibatches = []\n",
    "    Y_minibatches = [] \n",
    "    for i in range(numbatches):\n",
    "        idx_minibatch = idx[i*minibatchsize:min(len(idx),(i+1)*minibatchsize)]\n",
    "        xn = np.take(X,idx_minibatch,axis=0) \n",
    "        yn = np.take(Y,idx_minibatch,axis=0)\n",
    "        X_minibatches.append(xn)\n",
    "        Y_minibatches.append(yn)\n",
    "    X_minibatches, Y_minibatches = np.array(X_minibatches), np.array(Y_minibatches)\n",
    "    return X_minibatches, Y_minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Test mini-batches created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_create_minibatches():\n",
    "    X = []\n",
    "    Y = []\n",
    "    batch_size = 2\n",
    "    for i in range(20):\n",
    "        if(i % 2 == 0):\n",
    "            X.append([np.random.randint(1,10) for i1 in range(inputsize)])\n",
    "            Y.append(1)\n",
    "        else:\n",
    "            X.append([np.random.randint(-10,1) for i1 in range(inputsize)])\n",
    "            Y.append(0)\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    X_mb, Y_mb = create_minibatches(X,Y,batch_size)\n",
    "    print(X_mb, Y_mb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Generate random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def generateData(inputsize=3, onehotencoded=False, minibatch=False, batch_size=5):\n",
    "    X = []\n",
    "    Y = []\n",
    "    random.seed(0)\n",
    "    for i in range(500):\n",
    "        if(i % 2 == 0):\n",
    "            X.append([random.randint(1,10) for i1 in range(inputsize)])\n",
    "            Y.append(1)\n",
    "        else:\n",
    "            X.append([random.randint(-10,1) for i1 in range(inputsize)])\n",
    "            Y.append(0)\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    if onehotencoded:\n",
    "        Y = onehotencoding(Y)\n",
    "\n",
    "    if minibatch == False:\n",
    "        train_X = X\n",
    "        train_Y = Y\n",
    "    else:\n",
    "        train_X, train_Y = create_minibatches(X, Y, batch_size)\n",
    "    return train_X, train_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2(1)(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_2d():\n",
    "    #D_in is input dimension\n",
    "    #H1 is dimension of first hidden layer \n",
    "    #H2 is dimension of second hidden layer\n",
    "    #D_out is output dimension.\n",
    "    epochs = 1000\n",
    "    learning_rate = 0.001\n",
    "    inputsize = 3\n",
    "    loss_function = 'squared'\n",
    "    \n",
    "    onehotencoded = False\n",
    "    minibatch = False\n",
    "    train_X, train_Y = generateData(inputsize, onehotencoded, minibatch)\n",
    "    train_Y = train_Y.reshape((train_Y.shape[0], 1))\n",
    "    \n",
    "    print(train_X.shape, train_Y.shape)\n",
    "\n",
    "    D_in, H1, H2, H3, H4, D_out = inputsize, 10, 10, 10, 10, 1 \n",
    "    # list of number of neurons in the layers sequentially.\n",
    "    neurons = [D_in, H1, H2, H3, H4, D_out] \n",
    "    # activations in each layer (Note: the input layer does not have any activation)\n",
    "    activation_functions = ['linear','linear', 'tanh', 'relu', 'sigmoid'] \n",
    "\n",
    "    # Train the network\n",
    "    neuralnet = Neural_Network(neurons, activation_functions, epochs, learning_rate, loss_function)\n",
    "    loss = neuralnet.train(train_X, train_Y, minibatch=minibatch)\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.plot(loss)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Squared-error loss without mini-batch')\n",
    "    plt.show()\n",
    "\n",
    "plot_2d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2(1)(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_2e():\n",
    "    #D_in is input dimension\n",
    "    #H1 is dimension of first hidden layer \n",
    "    #H2 is dimension of second hidden layer\n",
    "    #D_out is output dimension.\n",
    "    epochs = 1000\n",
    "    learning_rate = 10 ** -3\n",
    "    inputsize = 3\n",
    "    loss_function = 'squared'\n",
    "\n",
    "    onehotencoded = False\n",
    "    minibatch = True\n",
    "    batch_size = 20\n",
    "    train_X, train_Y = generateData(inputsize, onehotencoded, minibatch, batch_size)\n",
    "    num_batches = train_X.shape[0]\n",
    "    train_Y = train_Y.reshape(num_batches, batch_size, 1)\n",
    "    \n",
    "    print(train_X.shape, train_Y.shape)\n",
    "\n",
    "    D_in, H1, H2, H3, H4, D_out = inputsize, 10, 10, 10, 10, 1 \n",
    "    # list of number of neurons in the layers sequentially.\n",
    "    neurons = [D_in, H1, H2, H3, H4, D_out] \n",
    "    # activations in each layer (Note: the input layer does not have any activation)\n",
    "    activation_functions = ['linear','linear', 'tanh', 'relu', 'sigmoid'] \n",
    "\n",
    "    # Train the network\n",
    "    neuralnet = Neural_Network(neurons, activation_functions, epochs, learning_rate, loss_function)\n",
    "    loss = neuralnet.train(train_X, train_Y, minibatch=minibatch)\n",
    "\n",
    "    # Plot the results\n",
    "    plt.plot(loss)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Squared-error loss with mini-batch size = 20')\n",
    "    plt.show()\n",
    "\n",
    "plot_2e()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Question 2(1)(f)\n",
    "### Observation \n",
    "Stochastic gradient descent performs better than mini-batch gradient descent.\n",
    "### Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Question 2(2)\n",
    "\n",
    "### Exploding and vanishing gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loadData_3():\n",
    "    data = datasets.load_digits()\n",
    "    return data['data'], data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def preprocess_3():\n",
    "    X, Y = loadData()\n",
    "    X = X / 255\n",
    "    return X, Y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Neural network with Squared-Error loss\n",
    "def train_3_se(batch_size=1, learning_rate=0.0001): \n",
    "    # Number of neurons in each layer\n",
    "    neurons = [64, 256, 128, 1]\n",
    "    activations = ['sigmoid', 'sigmoid', 'relu']\n",
    "    loss_function = 'squared'\n",
    "    epochs = 200\n",
    "\n",
    "    # Data preprocessing\n",
    "    onehotencoded = False\n",
    "    minibatch = True\n",
    "    X, Y = preprocess_3()\n",
    "    # Need to choose train set size based on the batch_sizes in question 3(e)\n",
    "    X, Y = X[:1500], Y[:1500]\n",
    "    X, Y = create_minibatches(X, Y, batch_size)\n",
    "    \n",
    "    # Split training and test data\n",
    "    num_training = 1500\n",
    "    num_batches = num_training // batch_size\n",
    "    train_X, train_Y = X[: num_batches], Y[: num_batches]    \n",
    "    train_Y = train_Y.reshape((num_batches, batch_size, 1))\n",
    "\n",
    "    print(train_X.shape, train_Y.shape)\n",
    "    \n",
    "    # Train the network\n",
    "    neuralnet = Neural_Network(neurons, activations, epochs, learning_rate, loss_function)\n",
    "    loss = neuralnet.train(train_X, train_Y, minibatch=minibatch)    \n",
    "    return neuralnet, loss\n",
    "    \n",
    "# neuralnet, loss = train_3_se([1], [0.0001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network with Cross-Entropy loss\n",
    "def train_3_ce(batch_size=1, learning_rate=0.0001):\n",
    "    # Number of neurons in each layer\n",
    "    neurons = [64, 256, 128, 10]\n",
    "    activations = ['sigmoid', 'sigmoid', 'softmax']\n",
    "    loss_function = 'cross-entropy'\n",
    "    epochs = 200\n",
    "\n",
    "    # Data preprocessing\n",
    "    onehotencoded = False\n",
    "    minibatch = True\n",
    "    X, Y = preprocess_3()\n",
    "    # Need to choose train set size based on the batch_sizes in question 3(e)\n",
    "    X, Y = X[:1500], Y[:1500]\n",
    "    Y = onehotencoding(Y)\n",
    "    X, Y = create_minibatches(X, Y, batch_size)\n",
    "\n",
    "    # Split training and test data\n",
    "    num_training = 1500\n",
    "    num_batches = num_training // batch_size\n",
    "    train_X, train_Y = X[: num_batches], Y[: num_batches]\n",
    "    train_Y = train_Y.reshape((num_batches, batch_size, 10))    \n",
    "\n",
    "    print(train_X.shape, train_Y.shape)\n",
    "    \n",
    "    # Train the network\n",
    "    neuralnet = Neural_Network(neurons, activations, epochs, learning_rate, loss_function)\n",
    "    loss = neuralnet.train(train_X, train_Y, minibatch=minibatch)\n",
    "    return neuralnet, loss\n",
    "\n",
    "# neuralnet, loss = train_3_ce()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Question 3(a)\n",
    "\n",
    "### Squared Error Loss\n",
    "\n",
    "Squared-error loss function is used in the regression scenario with number of output layer neurons = 1.\n",
    "This neuron predicts values in the range 0 to 9. The activation function used here is ReLU.\n",
    "\n",
    "### Cross-Entropy Loss\n",
    "CE loss function is used for multi-class classification and the number of output layer neurons in this case = 10.\n",
    "The output layer returns one-hot encoded values for labels in the range 0 to 9.\n",
    "Activation function used in this case is softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3(b)\n",
    "### Plot squared-error and cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3b():\n",
    "    neuralnet_se, loss_se = train_3_se()\n",
    "    neuralnet_ce, loss_ce = train_3_ce()\n",
    "    plt.ylabel(f'Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    \n",
    "    line, = plt.plot(loss_se, 'b')\n",
    "    line.set_label(f'Squared-error loss')\n",
    "    line, = plt.plot(loss_ce, 'r')\n",
    "    line.set_label(f'Cross-entropy loss')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_3b()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3(c)\n",
    "### Comparision of squared-error loss by varying learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3c():\n",
    "    batch_size = 1\n",
    "    lrs = [10 ** -1, 10 ** -2, 10 ** -3, 10 ** -5, 10 ** -6]\n",
    "    losses = []\n",
    "    for lr in lrs:\n",
    "        neuralnet, loss = train_3_se(batch_size, lr)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    # Plot the losses\n",
    "    colors = ['c', 'r', 'y', 'k', 'g']\n",
    "    for i in range(len(losses)):\n",
    "        line, = plt.plot(losses[i], colors[i])\n",
    "        line.set_label(f'Learning rate = {lrs[i]}')\n",
    "    \n",
    "    plt.ylabel('Squared-Error loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_3c()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3(d)\n",
    "### Comparision of cross-entropy loss by varying learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d():\n",
    "    batch_size = 1\n",
    "    lrs = [10 ** -1, 10 ** -2, 10 ** -3, 10 ** -5, 10 ** -6]\n",
    "    losses = []\n",
    "    for lr in lrs:\n",
    "        neuralnet, loss = train_3_ce(batch_size, lr)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    # Plot the losses\n",
    "    colors = ['c', 'r', 'y', 'k', 'g']\n",
    "    for i in range(len(losses)):\n",
    "        line, = plt.plot(losses[i], colors[i])\n",
    "        line.set_label(f'Learning rate = {lrs[i]}')\n",
    "    \n",
    "    plt.ylabel('Cross-entropy loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_3d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparision of squared-error loss by varying mini-batch size\n",
    "\n",
    "Learning rate = 10^(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3e_se():\n",
    "    lr = 10 ** -5\n",
    "    batch_sizes = [50, 100, 250, 300, 500]\n",
    "    losses = []\n",
    "    for batch_size in batch_sizes:\n",
    "        neuralnet, loss = train_3_se(batch_size, lr)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    # Plot the losses\n",
    "    colors = ['c', 'r', 'y', 'k', 'g']\n",
    "    for i in range(len(losses)):\n",
    "        line, = plt.plot(losses[i], colors[i])\n",
    "        line.set_label(f'Batch size = {batch_sizes[i]}')\n",
    "    \n",
    "    plt.ylabel('Squared-error loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_3e_se()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparision of cross-entropyloss by varying mini-batch size\n",
    "\n",
    "Learning rate = 10^(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3e_ce():\n",
    "    lr = 10 ** -5\n",
    "    batch_sizes = [50, 100, 250, 300, 500]\n",
    "    losses = []\n",
    "    for batch_size in batch_sizes:\n",
    "        neuralnet, loss = train_3_ce(batch_size, lr)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    # Plot the losses\n",
    "    colors = ['c', 'r', 'y', 'k', 'g']\n",
    "    for i in range(len(losses)):\n",
    "        line, = plt.plot(losses[i], colors[i])\n",
    "        line.set_label(f'Batch size = {batch_sizes[i]}')\n",
    "    \n",
    "    plt.ylabel('Squared-error loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_3e_ce()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Observation\n",
    "In both the cases, with increasing batch-size, performance degrades.\n",
    "\n",
    "### Reason\n",
    "The number of updates is inversely proportional to the batch-size.\n",
    "As the batch size increases, fewer corrections are made to the weights.\n",
    "This leads to underfitting and error decreases slowly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Question 4(a)\n",
    "### Split the data into S1 and S2 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def split_data_4a():\n",
    "    x, y = preprocess_3()\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(x)\n",
    "    np.random.shuffle(y)\n",
    "    num_samples = x.shape[0]\n",
    "    num_samples_s1 = int((num_samples * 0.8 // 50) * 50)\n",
    "    num_samples_s2 = int((num_samples * 0.2 // 50) * 50)\n",
    "    s1 = (x[:num_samples_s1], y[:num_samples_s1])\n",
    "    s2 = (x[:num_samples_s2], y[:num_samples_s2])\n",
    "    return (s1, s2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4(b)\n",
    "### Functions to compute squared-error and cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network with Squared-Error loss\n",
    "def train_4b_se(learning_rate=0.0001): \n",
    "    # Number of neurons in each layer\n",
    "    neurons = [64, 256, 128, 1]\n",
    "    activations = ['sigmoid', 'sigmoid', 'relu']\n",
    "    loss_function = 'squared'\n",
    "    epochs = 200\n",
    "\n",
    "    # Data preprocessing\n",
    "    onehotencoded = False\n",
    "    minibatch = True\n",
    "    batch_size = 50\n",
    "    samples = split_data_4a()\n",
    "\n",
    "    neuralnets = []\n",
    "    losses = []\n",
    "    for sample in samples:\n",
    "        X, Y = sample\n",
    "\n",
    "        # Create mini-batches\n",
    "        num_batches = int(X.shape[0] / batch_size)\n",
    "        train_X, train_Y = create_minibatches(X, Y, batch_size)\n",
    "        train_Y = train_Y.reshape((num_batches, batch_size, 1))\n",
    "\n",
    "        print(train_X.shape, train_Y.shape)\n",
    "\n",
    "         # Train the network\n",
    "        neuralnet = Neural_Network(neurons, activations, epochs, learning_rate, loss_function)\n",
    "        loss = neuralnet.train(train_X, train_Y, minibatch=minibatch)    \n",
    "        \n",
    "        neuralnets.append(neuralnet)\n",
    "        losses.append(loss)\n",
    "        \n",
    "    return neuralnets, losses\n",
    "\n",
    "# neuralnet, loss = train_4b_se()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network with Cross-Entropy loss\n",
    "def train_4b_ce(learning_rate=0.0001): \n",
    "    # Number of neurons in each layer\n",
    "    neurons = [64, 256, 128, 10]\n",
    "    activations = ['sigmoid', 'sigmoid', 'softmax']\n",
    "    loss_function = 'cross-entropy'\n",
    "    epochs = 200\n",
    "\n",
    "    # Data preprocessing\n",
    "    onehotencoded = False\n",
    "    minibatch = True\n",
    "    batch_size = 50\n",
    "    samples = split_data_4a()\n",
    "    \n",
    "    neuralnets = []\n",
    "    losses = []\n",
    "    for sample in samples:\n",
    "        X, Y = sample\n",
    "\n",
    "        # Create mini-batches\n",
    "        num_batches = int(X.shape[0] / batch_size)\n",
    "        train_X, train_Y = create_minibatches(X, Y, batch_size)\n",
    "        train_Y = train_Y.reshape((num_batches, batch_size, 1))\n",
    "\n",
    "        print(train_X.shape, train_Y.shape)\n",
    "\n",
    "         # Train the network\n",
    "        neuralnet = Neural_Network(neurons, activations, epochs, learning_rate, loss_function)\n",
    "        loss = neuralnet.train(train_X, train_Y, minibatch=minibatch)        \n",
    "        \n",
    "        neuralnets.append(neuralnet)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    return neuralnets, losses\n",
    "\n",
    "# neuralnet, loss = train_4b_ce()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4(c)\n",
    "### Selecting a suitable learning rate for squared-error loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_4c():\n",
    "    lrs = [10**-1, 10**-2, 10**-3, 10**-4, 10**-5, 10**-6]\n",
    "    losses_s1, losses_s2 = [], []\n",
    "    \n",
    "    for lr in lrs:\n",
    "        neuralnets, losses = train_4b_se(lr)\n",
    "        # Record every 5th epoch loss\n",
    "        losses_s1.append(losses[0][::5])\n",
    "        losses_s2.append(losses[1][::5])\n",
    "    \n",
    "    # Plot the losses for the two samples\n",
    "    colors = ['c', 'r', 'y', 'k', 'g', 'y']\n",
    "    for i in range(len(losses_s1)):\n",
    "        line, = plt.plot(losses_s1[i], colors[i])\n",
    "        line.set_label(f'Learning rate = {lrs[i]}')\n",
    "    plt.ylabel('Squared-error loss for S1')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    for i in range(len(losses_s2)):\n",
    "        line, = plt.plot(losses_s2[i], colors[i])\n",
    "        line.set_label(f'Learning rate = {lrs[i]}')\n",
    "    plt.ylabel('Squared-error loss for S2')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_4c()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4(d)\n",
    "### Selecting a suitable learning rate for cross-entropy loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_4c():\n",
    "    lrs = [10**-1, 10**-2, 10**-3, 10**-4, 10**-5, 10**-6]\n",
    "    losses_s1, losses_s2 = [], []\n",
    "    \n",
    "    for lr in lrs:\n",
    "        neuralnets, losses = train_4b_ce(lr)\n",
    "        # Record every 5th epoch loss\n",
    "        losses_s1.append(losses[0][::5])\n",
    "        losses_s2.append(losses[1][::5])\n",
    "    \n",
    "    # Plot the losses for the two samples\n",
    "    colors = ['c', 'r', 'y', 'k', 'g', 'y']\n",
    "    for i in range(len(losses_s1)):\n",
    "        line, = plt.plot(losses_s1[i], colors[i])\n",
    "        line.set_label(f'Learning rate = {lrs[i]}')\n",
    "    plt.ylabel('Squared-error loss for S1')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    for i in range(len(losses_s2)):\n",
    "        line, = plt.plot(losses_s2[i], colors[i])\n",
    "        line.set_label(f'Learning rate = {lrs[i]}')\n",
    "    plt.ylabel('Squared-error loss for S2')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_4c()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4(e)\n",
    "### Comparision between learning rates for 3(c), 3(d) with 4(c), 4(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
