{
 "cells": [
  {
   "source": [
    "<h1>Importing Libraries</h1>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "from sklearn import datasets"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 3,
   "outputs": []
  },
  {
   "source": [
    "<h1>Neural Network Implementation</h1>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network():\n",
    "    def __init__(self, neurons, Activations, epochs=1000, learning_rate=0.001, loss='squared'): \n",
    "        # arguments: an array \"neurons\" consist of number of neurons for each layer, \n",
    "        # an array \"activations\" consisting of activation functions used for the hidden layers and output layer\n",
    "        self.inputSize = neurons[0] # Number of neurons in input layer\n",
    "        self.outputSize = neurons[-1] # Number of neurons in output layer\n",
    "        self.layers = len(neurons)\n",
    "        self.weights = [] #weights for each layer\n",
    "        self.biases = [] #biases in each layer \n",
    "        self.layer_activations = [] #activations in each layer\n",
    "        self.epochs = epochs #number of epochs to train the network\n",
    "        self.learning_rate = learning_rate #learning rate used for training\n",
    "        self.loss_function = loss\n",
    "        for i in range(len(neurons)-1): \n",
    "            self.weights.append(np.random.rand(neurons[i+1],neurons[i])) #weight matrix between layer i and layer i+1\n",
    "            self.biases.append(np.random.rand(neurons[i+1],1))\n",
    "            self.layer_activations.append(Activations[i]) #activations for each layer\n",
    "        \n",
    "    def sigmoid(self, z): # sigmoid activation function\n",
    "        #Fill in the details to compute and return the sigmoid activation function                  \n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    def sigmoidPrime(self, z): # derivative of sigmoid activation function\n",
    "        #Fill in the details to compute and return the derivative of sigmoid activation function\n",
    "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
    "                          \n",
    "    def tanh(self, z): # hyperbolic tan activation function\n",
    "        #Fill in the details to compute and return the tanh activation function                  \n",
    "        exp_z = np.exp(z)\n",
    "        exp_z_negative = np.exp(-z)\n",
    "        return (exp_z - exp_z_negative) / (exp_z + exp_z_negative)  \n",
    "    \n",
    "    def tanhPrime(self, z): # derivative of hyperbolic tan activation function\n",
    "        #Fill in the details to compute and return the derivative of tanh activation function\n",
    "        return 1 - self.tanh(z) ** 2\n",
    "                          \n",
    "    def linear(self, z): # Linear activation function\n",
    "        #Fill in the details to compute and return the linear activation function                                    \n",
    "        return z\n",
    "    \n",
    "    def linearPrime(self, z): # derivative of linear activation function\n",
    "        #Fill in the details to compute and return the derivative of activation function                                                      \n",
    "        return np.ones(z.shape)\n",
    "\n",
    "    def ReLU(self, z): # ReLU activation function\n",
    "        #Fill in the details to compute and return the ReLU activation function                  \n",
    "        return np.where(z < 0, 0, z)\n",
    "    \n",
    "    def ReLUPrime(self, z): # derivative of ReLU activation function\n",
    "        #Fill in the details to compute and return the derivative of ReLU activation function\n",
    "        return np.where(z < 0, 0, 1)\n",
    "    \n",
    "    def forward(self, a): # function of forward pass which will receive input and give the output of final layer\n",
    "        # Write the forward pass using the weights and biases to find the predicted value and return them.\n",
    "        layer_activations_a = [a] #store the input as the input layer activations\n",
    "        layer_dot_prod_z = []\n",
    "        for i, param in enumerate(zip(self.biases, self.weights)):\n",
    "            b, w = param[0], param[1]\n",
    "            z = np.dot(w, a) + b\n",
    "            if self.layer_activations[i].lower()  == 'sigmoid':\n",
    "                a = self.sigmoid(z)\n",
    "            elif self.layer_activations[i].lower() == 'relu':\n",
    "                a = self.ReLU(z)    \n",
    "            elif self.layer_activations[i].lower() == 'tanh':   \n",
    "                a = self.tanh(z)\n",
    "            elif self.layer_activations[i].lower() == 'linear':\n",
    "                a = self.linear(z)\n",
    "            layer_dot_prod_z.append(z)    \n",
    "            layer_activations_a.append(a)\n",
    "        return a, layer_dot_prod_z, layer_activations_a\n",
    "        \n",
    "    def loss(self, Y_hat, Y):\n",
    "        #Implement the loss function\n",
    "        if self.loss_function.lower() == 'cross-entropy':\n",
    "            epsilon = 10 ** -5\n",
    "            Y_hat = np.where(Y_hat == 1, 1 - epsilon, Y_hat)\n",
    "            Y_hat = np.where(Y_hat == 0, epsilon, Y_hat)\n",
    "            error = - Y * np.log(Y_hat) - (1 - Y) * np.log(1 - Y_hat) \n",
    "        elif self.loss_function.lower() == 'squared':\n",
    "            error = 0.5 * (Y_hat - Y) ** 2\n",
    "        return error\n",
    "\n",
    "    def loss_grad(self, Y_hat, Y):\n",
    "        #Return the gradient of the loss function\n",
    "        if self.loss_function.lower() == 'cross-entropy':\n",
    "            epsilon = 10 ** -5\n",
    "            Y_hat = np.where(Y_hat == 1, 1 - epsilon, Y_hat)\n",
    "            Y_hat = np.where(Y_hat == 0, epsilon, Y_hat)\n",
    "            grad = - Y / Y_hat - (1 - Y) / (1 - Y_hat) \n",
    "        elif self.loss_function.lower() == 'squared':\n",
    "            grad = Y_hat - Y\n",
    "        return grad\n",
    "        \n",
    "    def backward(self, x, y, zs, activations): # find the loss and return derivative of loss w.r.t every parameter\n",
    "        # Write the backpropagation algorithm here to find the gradients of weights and biases and return them.\n",
    "        # Assuming L2 loss\n",
    "        grad_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        grad_w = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "        delta = self.loss_grad(activations[-1], y)\n",
    "        if self.layer_activations[-1].lower() == 'sigmoid':\n",
    "            delta = delta * self.sigmoidPrime(zs[-1])\n",
    "        elif self.layer_activations[-1].lower() == 'relu':\n",
    "            delta = delta * self.ReLUPrime(zs[-1])\n",
    "        elif self.layer_activations[-1].lower() == 'tanh':\n",
    "            delta = delta * self.tanhPrime(zs[-1])   \n",
    "        elif self.layer_activations[-1].lower() == 'linear':\n",
    "            delta = delta * self.linearPrime(zs[-1])\n",
    "        \n",
    "        # Number of training examples = m\n",
    "        m = delta.shape[1]\n",
    "        # fill in the appropriate details for gradients of w and b\n",
    "        grad_w[-1] = np.dot(delta, activations[-2].T) / m \n",
    "        grad_b[-1] = np.sum(delta, axis=1, keepdims=True) / m\n",
    "\n",
    "        for l in range(2, self.layers): # Here l is in backward sense i.e. last lth layer\n",
    "            z = zs[-l]\n",
    "            if self.layer_activations[-l].lower() == 'sigmoid':\n",
    "                prime = self.sigmoidPrime(z)\n",
    "            elif self.layer_activations[-l].lower() == 'relu':\n",
    "                prime = self.ReLUPrime(z)\n",
    "            elif self.layer_activations[-l].lower() == 'tanh':   \n",
    "                prime = self.tanhPrime(z)\n",
    "            elif self.layer_activations[-l].lower() == 'linear':\n",
    "                prime = self.linearPrime(z)\n",
    "\n",
    "            #Compute delta, gradients of b and w\n",
    "            delta = np.dot(self.weights[-l+1].T, delta) * prime\n",
    "            grad_w[-l] = np.dot(delta, activations[-l-1].T) / m \n",
    "            grad_b[-l] = np.sum(delta, axis=1, keepdims=True) / m\n",
    "\n",
    "        return (grad_b, grad_w)   \n",
    "\n",
    "    def update_parameters(self, grads, learning_rate): # update the parameters using the gradients\n",
    "        # update weights and biases using the gradients and the learning rate\n",
    "        grad_b, grad_w = grads[0], grads[1]       \n",
    "        \n",
    "        #Implement the update rule for weights  and biases\n",
    "        self.weights = [self.weights[i] - learning_rate * grad_w[i] for i in range(len(self.weights))]\n",
    "        self.biases = [self.biases[i] - learning_rate * grad_b[i] for i in range(len(self.biases))] \n",
    "\n",
    "    def train(self, X, Y, minibatch=False, batch_size=20): # receive the full training data set\n",
    "        lr = self.learning_rate # learning rate\n",
    "        epochs = self.epochs # number of epochs\n",
    "        loss_list = []\n",
    "        for e in range(epochs): \n",
    "            losses = []\n",
    "            for q in range(len(X)):\n",
    "                if minibatch == False:\n",
    "                    rows, cols = X[q].shape[0], 1\n",
    "                    train_x = np.resize(X[q],(rows, cols))\n",
    "                else:\n",
    "                    rows, cols = X[q].shape[1], X[q].shape[0]\n",
    "                    train_x = np.resize(X[q], (rows, cols))\n",
    "                \n",
    "                if not onehotencoded: \n",
    "                    train_y = np.resize(Y[q],(1,1)) \n",
    "                else:\n",
    "                    train_y = np.resize(np.argmax(Y[q]),(1,1)) \n",
    "                \n",
    "                out, dot_prod_z, activations_a = self.forward(train_x)\n",
    "                loss = self.loss(out, train_y)\n",
    "                grads = self.backward(train_x, train_y, dot_prod_z, activations_a) # find the gradients using backward pass\n",
    "                self.update_parameters(grads, lr)\n",
    "                losses.append(loss)\n",
    "            \n",
    "            loss_mean = np.mean(np.array(losses))\n",
    "            loss_list.append(loss_mean)\n",
    "            print(f'Epoch: {e} Loss: {loss_mean}')\n",
    "\n",
    "        return loss_list\n",
    "        \n",
    "    def predict(self, x):\n",
    "        print (\"Input : \\n\" + str(x))\n",
    "        prediction,_,_ = self.forward(x)\n",
    "        print (\"Output: \\n\" + str(prediction))\n"
   ]
  },
  {
   "source": [
    "# Miscellaneous Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### One-hot encoder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a method for creating one hot encoded labels \n",
    "def onehotencoding(Y):\n",
    "    rows = Y.shape[0]\n",
    "    values = {e:i for i, e in enumerate(np.unique(Y))}\n",
    "    y_enc = np.zeros((rows, len(values)))\n",
    "    for i in range(rows):\n",
    "        j = values[Y[i]]\n",
    "        y_enc[i][j] = 1\n",
    "    return y_enc"
   ]
  },
  {
   "source": [
    "### Create mini-batches"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a method to create minibatches \n",
    "def create_minibatches(X, Y, minibatchsize):\n",
    "    numbatches = int(np.ceil(len(X)/minibatchsize))\n",
    "    idx = np.arange(len(X))\n",
    "    np.random.shuffle(idx)\n",
    "    X_minibatches = []\n",
    "    Y_minibatches = [] \n",
    "    for i in range(numbatches):\n",
    "        idx_minibatch = idx[i*minibatchsize:min(len(idx),(i+1)*minibatchsize)]\n",
    "        xn = np.take(X,idx_minibatch,axis=0) \n",
    "        yn = np.take(Y,idx_minibatch,axis=0)\n",
    "        X_minibatches.append(xn)\n",
    "        Y_minibatches.append(yn)\n",
    "    X_minibatches, Y_minibatches = np.array(X_minibatches), np.array(Y_minibatches)\n",
    "    return X_minibatches, Y_minibatches"
   ]
  },
  {
   "source": [
    "### Test mini-batches created"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_create_minibatches():\n",
    "    X = []\n",
    "    Y = []\n",
    "    batch_size = 2\n",
    "    for i in range(20):\n",
    "        if(i % 2 == 0):\n",
    "            X.append([np.random.randint(1,10) for i1 in range(inputsize)])\n",
    "            Y.append(1)\n",
    "        else:\n",
    "            X.append([np.random.randint(-10,1) for i1 in range(inputsize)])\n",
    "            Y.append(0)\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    X_mb, Y_mb = create_minibatches(X,Y,batch_size)\n",
    "    print(X_mb, Y_mb)\n"
   ]
  },
  {
   "source": [
    "### Generate random data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateData(inputsize=3, onehotencoded=False, minibatch=False, batch_size=5):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in range(500):\n",
    "        if(i % 2 == 0):\n",
    "            X.append([random.randint(1,10) for i1 in range(inputsize)])\n",
    "            Y.append(1)\n",
    "        else:\n",
    "            X.append([random.randint(-10,1) for i1 in range(inputsize)])\n",
    "            Y.append(0)\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    if onehotencoded:\n",
    "        Y = onehotencoding(Y)\n",
    "\n",
    "    if minibatch == False:\n",
    "        train_X = X\n",
    "        train_Y = Y\n",
    "    else:\n",
    "        train_X, train_Y = create_minibatches(X, Y, batch_size)\n",
    "    return train_X, train_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2(1)(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def question_2d():\n",
    "    #D_in is input dimension\n",
    "    #H1 is dimension of first hidden layer \n",
    "    #H2 is dimension of second hidden layer\n",
    "    #D_out is output dimension.\n",
    "    epochs = 20\n",
    "    learning_rate = 0.001\n",
    "    inputsize = 3\n",
    "    loss_function = 'squared'\n",
    "    \n",
    "    onehotencoded = False\n",
    "    minibatch = False\n",
    "    train_X, train_Y = generateData(inputsize, onehotencoded, minibatch)\n",
    "\n",
    "    D_in, H1, H2, H3, H4, D_out = inputsize, 10, 5, 10, 5, 1 \n",
    "    # list of number of neurons in the layers sequentially.\n",
    "    neurons = [D_in, H1, H2, H3, H4, D_out] \n",
    "    # activations in each layer (Note: the input layer does not have any activation)\n",
    "    activation_functions = ['linear','linear', 'tanh', 'relu', 'sigmoid'] \n",
    "\n",
    "    # Train the network\n",
    "    neuralnet = Neural_Network(neurons, activation_functions, epochs, learning_rate, loss_function)\n",
    "    loss = neuralnet.train(train_X, train_Y, minibatch=minibatch)\n",
    "    \n",
    "    # Predict on the trained network\n",
    "    neuralnet.predict(np.array([8,4,9]).reshape((3,1)))\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.plot(loss)\n",
    "    plt.show()\n",
    "\n",
    "# question_2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Question 2(1)(e)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def question_2e():\n",
    "    #D_in is input dimension\n",
    "    #H1 is dimension of first hidden layer \n",
    "    #H2 is dimension of second hidden layer\n",
    "    #D_out is output dimension.\n",
    "    epochs = 100\n",
    "    learning_rate = 0.001\n",
    "    inputsize = 3\n",
    "    loss_function = 'squared'\n",
    "\n",
    "    onehotencoded = False\n",
    "    minibatch = True\n",
    "    batch_size = 20\n",
    "    train_X, train_Y = generateData(inputsize, onehotencoded, minibatch, batch_size)\n",
    "    print(train_X.shape, train_Y.shape)\n",
    "\n",
    "    D_in, H1, H2, H3, H4, D_out = inputsize, 10, 5, 10, 5, 1 \n",
    "    # list of number of neurons in the layers sequentially.\n",
    "    neurons = [D_in, H1, H2, H3, H4, D_out] \n",
    "    # activations in each layer (Note: the input layer does not have any activation)\n",
    "    activation_functions = ['linear','linear', 'tanh', 'relu', 'sigmoid'] \n",
    "\n",
    "    # Train the network\n",
    "    neuralnet = Neural_Network(neurons, activation_functions, epochs, learning_rate, loss_function)\n",
    "    loss = neuralnet.train(train_X, train_Y, minibatch=minibatch)\n",
    "    \n",
    "    # Predict on the trained network\n",
    "    neuralnet.predict(np.array([8,4,9]).reshape((3,1)))\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.plot(loss)\n",
    "    plt.show()\n",
    "\n",
    "# question_2e()"
   ]
  },
  {
   "source": [
    "# Question 2(1)(f)\n",
    "### Observation \n",
    "Stochastic gradient descent performs better than mini-batch gradient descent.\n",
    "### Why?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Question 2(2)\n",
    "\n",
    "### Exploding and vanishing gradients"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Question 3\n",
    "### Import dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loadData():\n",
    "    data = datasets.load_digits()\n",
    "    return data['data'], data['target']"
   ]
  },
  {
   "source": [
    "### Create a neural network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(1500, 1, 64) (1500, 1, 10)\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-004eddc02660>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m  \u001b[0;31m# Train the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mneuralnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeural_Network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneurons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneuralnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Plot the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-71687b941e3f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, Y, minibatch, batch_size)\u001b[0m\n\u001b[1;32m    159\u001b[0m                     \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdot_prod_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdot_prod_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations_a\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# find the gradients using backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-71687b941e3f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_activations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;34m==\u001b[0m \u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Number of neurons in each layer\n",
    "neurons = [64, 256, 128, 10]\n",
    "activations = ['sigmoid', 'sigmoid', 'sigmoid']\n",
    "loss_function = 'squared'\n",
    "epochs = 1000\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Data preprocessing\n",
    "X,Y = loadData()\n",
    "onehotencoded = False\n",
    "minibatch = True\n",
    "batch_size = 1\n",
    "Y = onehotencoding(Y)\n",
    "X, Y = create_minibatches(X, Y, batch_size)\n",
    "\n",
    "# Split training and test data\n",
    "num_training = 1500\n",
    "train_X, train_Y = X[: num_training], Y[: num_training]\n",
    "test_X, test_Y = X[num_training:], Y[num_training:]\n",
    "\n",
    "print(train_X.shape, train_Y.shape)\n",
    "\n",
    " # Train the network\n",
    "neuralnet = Neural_Network(neurons, activations, epochs, learning_rate, loss_function)\n",
    "loss = neuralnet.train(train_X, train_Y, minibatch=minibatch)\n",
    "\n",
    "# Plot the results\n",
    "plt.plot(loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}