{
 "cells": [
  {
   "source": [
    "<h1>Importing Libraries</h1>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "from sklearn import datasets"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": []
  },
  {
   "source": [
    "<h1>Neural Network Implementation</h1>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network():\n",
    "    def __init__(self, neurons, Activations, epochs=1000, learning_rate=0.001, loss='squared'): \n",
    "        # arguments: an array \"neurons\" consist of number of neurons for each layer, \n",
    "        # an array \"activations\" consisting of activation functions used for the hidden layers and output layer\n",
    "        self.inputSize = neurons[0] # Number of neurons in input layer\n",
    "        self.outputSize = neurons[-1] # Number of neurons in output layer\n",
    "        self.layers = len(neurons)\n",
    "        self.weights = [] #weights for each layer\n",
    "        self.biases = [] #biases in each layer \n",
    "        self.layer_activations = [] #activations in each layer\n",
    "        self.epochs = epochs #number of epochs to train the network\n",
    "        self.learning_rate = learning_rate #learning rate used for training\n",
    "        self.loss_function = loss\n",
    "        for i in range(len(neurons)-1): \n",
    "            self.weights.append(np.random.rand(neurons[i+1],neurons[i])) #weight matrix between layer i and layer i+1\n",
    "            self.biases.append(np.random.rand(neurons[i+1],1))\n",
    "            self.layer_activations.append(Activations[i]) #activations for each layer\n",
    "        \n",
    "    def sigmoid(self, z): # sigmoid activation function\n",
    "        #Fill in the details to compute and return the sigmoid activation function                  \n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    def sigmoidPrime(self, z): # derivative of sigmoid activation function\n",
    "        #Fill in the details to compute and return the derivative of sigmoid activation function\n",
    "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
    "                          \n",
    "    def tanh(self, z): # hyperbolic tan activation function\n",
    "        #Fill in the details to compute and return the tanh activation function                  \n",
    "        exp_z = np.exp(z)\n",
    "        exp_z_negative = np.exp(-z)\n",
    "        return (exp_z - exp_z_negative) / (exp_z + exp_z_negative)  \n",
    "    \n",
    "    def tanhPrime(self, z): # derivative of hyperbolic tan activation function\n",
    "        #Fill in the details to compute and return the derivative of tanh activation function\n",
    "        return 1 - self.tanh(z) ** 2\n",
    "                          \n",
    "    def linear(self, z): # Linear activation function\n",
    "        #Fill in the details to compute and return the linear activation function                                    \n",
    "        return z\n",
    "    \n",
    "    def linearPrime(self, z): # derivative of linear activation function\n",
    "        #Fill in the details to compute and return the derivative of activation function                                                      \n",
    "        return np.ones(z.shape)\n",
    "\n",
    "    def ReLU(self, z): # ReLU activation function\n",
    "        #Fill in the details to compute and return the ReLU activation function                  \n",
    "        return np.where(z < 0, 0, z)\n",
    "    \n",
    "    def ReLUPrime(self, z): # derivative of ReLU activation function\n",
    "        #Fill in the details to compute and return the derivative of ReLU activation function\n",
    "        return np.where(z < 0, 0, 1)\n",
    "    \n",
    "    def forward(self, a): # function of forward pass which will receive input and give the output of final layer\n",
    "        # Write the forward pass using the weights and biases to find the predicted value and return them.\n",
    "        layer_activations_a = [a] #store the input as the input layer activations\n",
    "        layer_dot_prod_z = []\n",
    "        for i, param in enumerate(zip(self.biases, self.weights)):\n",
    "            b, w = param[0], param[1]\n",
    "            z = np.dot(w, a) + b\n",
    "            if self.layer_activations[i].lower()  == 'sigmoid':\n",
    "                a = self.sigmoid(z)\n",
    "            elif self.layer_activations[i].lower() == 'relu':\n",
    "                a = self.ReLU(z)    \n",
    "            elif self.layer_activations[i].lower() == 'tanh':   \n",
    "                a = self.tanh(z)\n",
    "            elif self.layer_activations[i].lower() == 'linear':\n",
    "                a = self.linear(z)\n",
    "            layer_dot_prod_z.append(z)    \n",
    "            layer_activations_a.append(a)\n",
    "        return a, layer_dot_prod_z, layer_activations_a\n",
    "        \n",
    "    def loss(self, Y_hat, Y):\n",
    "        #Implement the loss function\n",
    "        if self.loss_function.lower() == 'cross-entropy':\n",
    "            epsilon = 10 ** -5\n",
    "            Y_hat = np.where(Y_hat == 1, 1 - epsilon, Y_hat)\n",
    "            Y_hat = np.where(Y_hat == 0, epsilon, Y_hat)\n",
    "            error = - Y * np.log(Y_hat) - (1 - Y) * np.log(1 - Y_hat) \n",
    "        elif self.loss_function.lower() == 'squared':\n",
    "            error = 0.5 * (Y_hat - Y) ** 2\n",
    "        return error\n",
    "\n",
    "    def loss_grad(self, Y_hat, Y):\n",
    "        #Return the gradient of the loss function\n",
    "        if self.loss_function.lower() == 'cross-entropy':\n",
    "            epsilon = 10 ** -5\n",
    "            Y_hat = np.where(Y_hat == 1, 1 - epsilon, Y_hat)\n",
    "            Y_hat = np.where(Y_hat == 0, epsilon, Y_hat)\n",
    "            grad = - Y / Y_hat - (1 - Y) / (1 - Y_hat) \n",
    "        elif self.loss_function.lower() == 'squared':\n",
    "            grad = Y_hat - Y\n",
    "        return grad\n",
    "        \n",
    "    def backward(self, x, y, zs, activations): # find the loss and return derivative of loss w.r.t every parameter\n",
    "        # Write the backpropagation algorithm here to find the gradients of weights and biases and return them.\n",
    "        # Assuming L2 loss\n",
    "        grad_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        grad_w = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "        delta = self.loss_grad(activations[-1], y)\n",
    "        if self.layer_activations[-1].lower() == 'sigmoid':\n",
    "            delta = delta * self.sigmoidPrime(zs[-1])\n",
    "        elif self.layer_activations[-1].lower() == 'relu':\n",
    "            delta = delta * self.ReLUPrime(zs[-1])\n",
    "        elif self.layer_activations[-1].lower() == 'tanh':\n",
    "            delta = delta * self.tanhPrime(zs[-1])   \n",
    "        elif self.layer_activations[-1].lower() == 'linear':\n",
    "            delta = delta * self.linearPrime(zs[-1])\n",
    "        \n",
    "        # Number of training examples = m\n",
    "        m = delta.shape[1]\n",
    "        # fill in the appropriate details for gradients of w and b\n",
    "        grad_w[-1] = np.dot(delta, activations[-2].T) / m \n",
    "        grad_b[-1] = np.sum(delta, axis=1, keepdims=True) / m\n",
    "\n",
    "        for l in range(2, self.layers): # Here l is in backward sense i.e. last lth layer\n",
    "            z = zs[-l]\n",
    "            if self.layer_activations[-l].lower() == 'sigmoid':\n",
    "                prime = self.sigmoidPrime(z)\n",
    "            elif self.layer_activations[-l].lower() == 'relu':\n",
    "                prime = self.ReLUPrime(z)\n",
    "            elif self.layer_activations[-l].lower() == 'tanh':   \n",
    "                prime = self.tanhPrime(z)\n",
    "            elif self.layer_activations[-l].lower() == 'linear':\n",
    "                prime = self.linearPrime(z)\n",
    "\n",
    "            #Compute delta, gradients of b and w\n",
    "            delta = np.dot(self.weights[-l+1].T, delta) * prime\n",
    "            grad_w[-l] = np.dot(delta, activations[-l-1].T) / m \n",
    "            grad_b[-l] = np.sum(delta, axis=1, keepdims=True) / m\n",
    "\n",
    "        return (grad_b, grad_w)   \n",
    "\n",
    "    def update_parameters(self, grads, learning_rate): # update the parameters using the gradients\n",
    "        # update weights and biases using the gradients and the learning rate\n",
    "        grad_b, grad_w = grads[0], grads[1]       \n",
    "        \n",
    "        #Implement the update rule for weights  and biases\n",
    "        self.weights = [self.weights[i] - learning_rate * grad_w[i] for i in range(len(self.weights))]\n",
    "        self.biases = [self.biases[i] - learning_rate * grad_b[i] for i in range(len(self.biases))] \n",
    "\n",
    "    def train(self, X, Y, minibatch=False, batch_size=20): # receive the full training data set\n",
    "        lr = self.learning_rate # learning rate\n",
    "        epochs = self.epochs # number of epochs\n",
    "        loss_list = []\n",
    "        num_updates = 0 # number of updates\n",
    "        for e in range(epochs): \n",
    "            losses = []\n",
    "            for q in range(len(X)):\n",
    "                if minibatch == False:\n",
    "                    rows, cols = X[q].shape[0], 1\n",
    "                else:\n",
    "                    rows, cols = X[q].shape[1], X[q].shape[0]\n",
    "                train_x = np.resize(X[q], (rows, cols))\n",
    "                \n",
    "                if not onehotencoded: \n",
    "                    train_y = np.resize(Y[q],(1,1)) \n",
    "                else:\n",
    "                    train_y = np.resize(np.argmax(Y[q]),(1,1)) \n",
    "                \n",
    "                out, dot_prod_z, activations_a = self.forward(train_x)\n",
    "                loss = self.loss(out, train_y)\n",
    "                grads = self.backward(train_x, train_y, dot_prod_z, activations_a) # find the gradients using backward pass\n",
    "                self.update_parameters(grads, lr)\n",
    "                num_updates += 1\n",
    "                losses.append(loss)\n",
    "            \n",
    "            loss_mean = np.mean(np.array(losses))\n",
    "            loss_list.append(loss_mean)\n",
    "            print(f'Epoch: {e} Loss: {loss_mean}')\n",
    "            print(f'Epoch: {e} Number of updates: {num_updates}')\n",
    "            num_updates = 0\n",
    "        return loss_list\n",
    "        \n",
    "    def predict(self, x):\n",
    "        print (\"Input : \\n\" + str(x))\n",
    "        prediction,_,_ = self.forward(x)\n",
    "        print (\"Output: \\n\" + str(prediction))\n"
   ]
  },
  {
   "source": [
    "# Miscellaneous Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### One-hot encoder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a method for creating one hot encoded labels \n",
    "def onehotencoding(Y):\n",
    "    rows = Y.shape[0]\n",
    "    values = {e:i for i, e in enumerate(np.unique(Y))}\n",
    "    y_enc = np.zeros((rows, len(values)))\n",
    "    for i in range(rows):\n",
    "        j = values[Y[i]]\n",
    "        y_enc[i][j] = 1\n",
    "    return y_enc"
   ]
  },
  {
   "source": [
    "### Create mini-batches"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a method to create minibatches \n",
    "def create_minibatches(X, Y, minibatchsize):\n",
    "    numbatches = int(np.ceil(len(X)/minibatchsize))\n",
    "    idx = np.arange(len(X))\n",
    "    np.random.shuffle(idx)\n",
    "    X_minibatches = []\n",
    "    Y_minibatches = [] \n",
    "    for i in range(numbatches):\n",
    "        idx_minibatch = idx[i*minibatchsize:min(len(idx),(i+1)*minibatchsize)]\n",
    "        xn = np.take(X,idx_minibatch,axis=0) \n",
    "        yn = np.take(Y,idx_minibatch,axis=0)\n",
    "        X_minibatches.append(xn)\n",
    "        Y_minibatches.append(yn)\n",
    "    X_minibatches, Y_minibatches = np.array(X_minibatches), np.array(Y_minibatches)\n",
    "    return X_minibatches, Y_minibatches"
   ]
  },
  {
   "source": [
    "### Test mini-batches created"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_create_minibatches():\n",
    "    X = []\n",
    "    Y = []\n",
    "    batch_size = 2\n",
    "    for i in range(20):\n",
    "        if(i % 2 == 0):\n",
    "            X.append([np.random.randint(1,10) for i1 in range(inputsize)])\n",
    "            Y.append(1)\n",
    "        else:\n",
    "            X.append([np.random.randint(-10,1) for i1 in range(inputsize)])\n",
    "            Y.append(0)\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    X_mb, Y_mb = create_minibatches(X,Y,batch_size)\n",
    "    print(X_mb, Y_mb)\n"
   ]
  },
  {
   "source": [
    "### Generate random data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateData(inputsize=3, onehotencoded=False, minibatch=False, batch_size=5):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in range(500):\n",
    "        if(i % 2 == 0):\n",
    "            X.append([random.randint(1,10) for i1 in range(inputsize)])\n",
    "            Y.append(1)\n",
    "        else:\n",
    "            X.append([random.randint(-10,1) for i1 in range(inputsize)])\n",
    "            Y.append(0)\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    if onehotencoded:\n",
    "        Y = onehotencoding(Y)\n",
    "\n",
    "    if minibatch == False:\n",
    "        train_X = X\n",
    "        train_Y = Y\n",
    "    else:\n",
    "        train_X, train_Y = create_minibatches(X, Y, batch_size)\n",
    "    return train_X, train_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2(1)(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def question_2d():\n",
    "    #D_in is input dimension\n",
    "    #H1 is dimension of first hidden layer \n",
    "    #H2 is dimension of second hidden layer\n",
    "    #D_out is output dimension.\n",
    "    epochs = 20\n",
    "    learning_rate = 0.001\n",
    "    inputsize = 3\n",
    "    loss_function = 'squared'\n",
    "    \n",
    "    onehotencoded = False\n",
    "    minibatch = False\n",
    "    train_X, train_Y = generateData(inputsize, onehotencoded, minibatch)\n",
    "\n",
    "    D_in, H1, H2, H3, H4, D_out = inputsize, 10, 5, 10, 5, 1 \n",
    "    # list of number of neurons in the layers sequentially.\n",
    "    neurons = [D_in, H1, H2, H3, H4, D_out] \n",
    "    # activations in each layer (Note: the input layer does not have any activation)\n",
    "    activation_functions = ['linear','linear', 'tanh', 'relu', 'sigmoid'] \n",
    "\n",
    "    # Train the network\n",
    "    neuralnet = Neural_Network(neurons, activation_functions, epochs, learning_rate, loss_function)\n",
    "    loss = neuralnet.train(train_X, train_Y, minibatch=minibatch)\n",
    "    \n",
    "    # Predict on the trained network\n",
    "    neuralnet.predict(np.array([8,4,9]).reshape((3,1)))\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.plot(loss)\n",
    "    plt.show()\n",
    "\n",
    "# question_2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Question 2(1)(e)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def question_2e():\n",
    "    #D_in is input dimension\n",
    "    #H1 is dimension of first hidden layer \n",
    "    #H2 is dimension of second hidden layer\n",
    "    #D_out is output dimension.\n",
    "    epochs = 100\n",
    "    learning_rate = 0.001\n",
    "    inputsize = 3\n",
    "    loss_function = 'squared'\n",
    "\n",
    "    onehotencoded = False\n",
    "    minibatch = True\n",
    "    batch_size = 20\n",
    "    train_X, train_Y = generateData(inputsize, onehotencoded, minibatch, batch_size)\n",
    "    print(train_X.shape, train_Y.shape)\n",
    "\n",
    "    D_in, H1, H2, H3, H4, D_out = inputsize, 10, 5, 10, 5, 1 \n",
    "    # list of number of neurons in the layers sequentially.\n",
    "    neurons = [D_in, H1, H2, H3, H4, D_out] \n",
    "    # activations in each layer (Note: the input layer does not have any activation)\n",
    "    activation_functions = ['linear','linear', 'tanh', 'relu', 'sigmoid'] \n",
    "\n",
    "    # Train the network\n",
    "    neuralnet = Neural_Network(neurons, activation_functions, epochs, learning_rate, loss_function)\n",
    "    loss = neuralnet.train(train_X, train_Y, minibatch=minibatch)\n",
    "    \n",
    "    # Predict on the trained network\n",
    "    neuralnet.predict(np.array([8,4,9]).reshape((3,1)))\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.plot(loss)\n",
    "    plt.show()\n",
    "\n",
    "# question_2e()"
   ]
  },
  {
   "source": [
    "# Question 2(1)(f)\n",
    "### Observation \n",
    "Stochastic gradient descent performs better than mini-batch gradient descent.\n",
    "### Why?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Question 2(2)\n",
    "\n",
    "### Exploding and vanishing gradients"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Question 3\n",
    "### Import dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loadData():\n",
    "    data = datasets.load_digits()\n",
    "    return data['data'], data['target']"
   ]
  },
  {
   "source": [
    "### Data preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    X, Y = loadData()\n",
    "    X = X / 255\n",
    "    return X, Y\n",
    "    "
   ]
  },
  {
   "source": [
    "### Create a neural network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(1,) (1,)\nEpoch: 0 Loss: 2081.7102764365145\nEpoch: 0 Number of updates: 1\nEpoch: 1 Loss: 2028.3485687115547\nEpoch: 1 Number of updates: 1\nEpoch: 2 Loss: 1976.354713124116\nEpoch: 2 Number of updates: 1\nEpoch: 3 Loss: 1925.6936467133241\nEpoch: 3 Number of updates: 1\nEpoch: 4 Loss: 1876.3312053078698\nEpoch: 4 Number of updates: 1\nEpoch: 5 Loss: 1828.2341004868026\nEpoch: 5 Number of updates: 1\nEpoch: 6 Loss: 1781.3698971309054\nEpoch: 6 Number of updates: 1\nEpoch: 7 Loss: 1735.7069915495092\nEpoch: 7 Number of updates: 1\nEpoch: 8 Loss: 1691.2145901679962\nEpoch: 8 Number of updates: 1\nEpoch: 9 Loss: 1647.862688761611\nEpoch: 9 Number of updates: 1\nEpoch: 10 Loss: 1605.6220522215983\nEpoch: 10 Number of updates: 1\nEpoch: 11 Loss: 1564.464194839992\nEpoch: 11 Number of updates: 1\nEpoch: 12 Loss: 1524.3613610997834\nEpoch: 12 Number of updates: 1\nEpoch: 13 Loss: 1485.2865069575096\nEpoch: 13 Number of updates: 1\nEpoch: 14 Loss: 1447.2132816056285\nEpoch: 14 Number of updates: 1\nEpoch: 15 Loss: 1410.116009702395\nEpoch: 15 Number of updates: 1\nEpoch: 16 Loss: 1373.9696740572476\nEpoch: 16 Number of updates: 1\nEpoch: 17 Loss: 1338.7498987600306\nEpoch: 17 Number of updates: 1\nEpoch: 18 Loss: 1304.4329327426747\nEpoch: 18 Number of updates: 1\nEpoch: 19 Loss: 1270.9956337622518\nEpoch: 19 Number of updates: 1\nEpoch: 20 Loss: 1238.4154527945993\nEpoch: 20 Number of updates: 1\nEpoch: 21 Loss: 1206.6704188279982\nEpoch: 21 Number of updates: 1\nEpoch: 22 Loss: 1175.7391240466327\nEpoch: 22 Number of updates: 1\nEpoch: 23 Loss: 1145.6007093938629\nEpoch: 23 Number of updates: 1\nEpoch: 24 Loss: 1116.2348505055513\nEpoch: 24 Number of updates: 1\nEpoch: 25 Loss: 1087.6217440039807\nEpoch: 25 Number of updates: 1\nEpoch: 26 Loss: 1059.7420941430978\nEpoch: 26 Number of updates: 1\nEpoch: 27 Loss: 1032.577099796092\nEpoch: 27 Number of updates: 1\nEpoch: 28 Loss: 1006.1084417765304\nEpoch: 28 Number of updates: 1\nEpoch: 29 Loss: 980.3182704844919\nEpoch: 29 Number of updates: 1\nEpoch: 30 Loss: 955.1891938693832\nEpoch: 30 Number of updates: 1\nEpoch: 31 Loss: 930.7042657013045\nEpoch: 31 Number of updates: 1\nEpoch: 32 Loss: 906.8469741430665\nEpoch: 32 Number of updates: 1\nEpoch: 33 Loss: 883.6012306151426\nEpoch: 33 Number of updates: 1\nEpoch: 34 Loss: 860.9513589460583\nEpoch: 34 Number of updates: 1\nEpoch: 35 Loss: 838.8820848008924\nEpoch: 35 Number of updates: 1\nEpoch: 36 Loss: 817.378525380761\nEpoch: 36 Number of updates: 1\nEpoch: 37 Loss: 796.4261793863459\nEpoch: 37 Number of updates: 1\nEpoch: 38 Loss: 776.0109172386897\nEpoch: 38 Number of updates: 1\nEpoch: 39 Loss: 756.1189715506698\nEpoch: 39 Number of updates: 1\nEpoch: 40 Loss: 736.7369278427178\nEpoch: 40 Number of updates: 1\nEpoch: 41 Loss: 717.8517154965381\nEpoch: 41 Number of updates: 1\nEpoch: 42 Loss: 699.450598940703\nEpoch: 42 Number of updates: 1\nEpoch: 43 Loss: 681.5211690622027\nEpoch: 43 Number of updates: 1\nEpoch: 44 Loss: 664.0513348381418\nEpoch: 44 Number of updates: 1\nEpoch: 45 Loss: 647.0293151819478\nEpoch: 45 Number of updates: 1\nEpoch: 46 Loss: 630.443630998593\nEpoch: 46 Number of updates: 1\nEpoch: 47 Loss: 614.2830974434637\nEpoch: 47 Number of updates: 1\nEpoch: 48 Loss: 598.536816379668\nEpoch: 48 Number of updates: 1\nEpoch: 49 Loss: 583.1941690286862\nEpoch: 49 Number of updates: 1\nEpoch: 50 Loss: 568.2448088094143\nEpoch: 50 Number of updates: 1\nEpoch: 51 Loss: 553.6786543607653\nEpoch: 51 Number of updates: 1\nEpoch: 52 Loss: 539.4858827431297\nEpoch: 52 Number of updates: 1\nEpoch: 53 Loss: 525.6569228141044\nEpoch: 53 Number of updates: 1\nEpoch: 54 Loss: 512.1824487740259\nEpoch: 54 Number of updates: 1\nEpoch: 55 Loss: 499.05337387695647\nEpoch: 55 Number of updates: 1\nEpoch: 56 Loss: 486.26084430287807\nEpoch: 56 Number of updates: 1\nEpoch: 57 Loss: 473.7962331869642\nEpoch: 57 Number of updates: 1\nEpoch: 58 Loss: 461.65113480190513\nEpoch: 58 Number of updates: 1\nEpoch: 59 Loss: 449.8173588893585\nEpoch: 59 Number of updates: 1\nEpoch: 60 Loss: 438.2869251367059\nEpoch: 60 Number of updates: 1\nEpoch: 61 Loss: 427.0520577953907\nEpoch: 61 Number of updates: 1\nEpoch: 62 Loss: 416.1051804372074\nEpoch: 62 Number of updates: 1\nEpoch: 63 Loss: 405.43891084500405\nEpoch: 63 Number of updates: 1\nEpoch: 64 Loss: 395.04605603435675\nEpoch: 64 Number of updates: 1\nEpoch: 65 Loss: 384.91960740285487\nEpoch: 65 Number of updates: 1\nEpoch: 66 Loss: 375.0527360037291\nEpoch: 66 Number of updates: 1\nEpoch: 67 Loss: 365.4387879406313\nEpoch: 67 Number of updates: 1\nEpoch: 68 Loss: 356.07127988046415\nEpoch: 68 Number of updates: 1\nEpoch: 69 Loss: 346.943894681233\nEpoch: 69 Number of updates: 1\nEpoch: 70 Loss: 338.0504771319712\nEpoch: 70 Number of updates: 1\nEpoch: 71 Loss: 329.3850298018659\nEpoch: 71 Number of updates: 1\nEpoch: 72 Loss: 320.94170899578705\nEpoch: 72 Number of updates: 1\nEpoch: 73 Loss: 312.7148208134898\nEpoch: 73 Number of updates: 1\nEpoch: 74 Loss: 304.69881730983326\nEpoch: 74 Number of updates: 1\nEpoch: 75 Loss: 296.8882927534282\nEpoch: 75 Number of updates: 1\nEpoch: 76 Loss: 289.27797998118666\nEpoch: 76 Number of updates: 1\nEpoch: 77 Loss: 281.8627468463207\nEpoch: 77 Number of updates: 1\nEpoch: 78 Loss: 274.6375927573884\nEpoch: 78 Number of updates: 1\nEpoch: 79 Loss: 267.5976453060585\nEpoch: 79 Number of updates: 1\nEpoch: 80 Loss: 260.7381569813176\nEpoch: 80 Number of updates: 1\nEpoch: 81 Loss: 254.0545019679029\nEpoch: 81 Number of updates: 1\nEpoch: 82 Loss: 247.5421730268035\nEpoch: 82 Number of updates: 1\nEpoch: 83 Loss: 241.1967784557253\nEpoch: 83 Number of updates: 1\nEpoch: 84 Loss: 235.01403912747054\nEpoch: 84 Number of updates: 1\nEpoch: 85 Loss: 228.989785604233\nEpoch: 85 Number of updates: 1\nEpoch: 86 Loss: 223.11995532586604\nEpoch: 86 Number of updates: 1\nEpoch: 87 Loss: 217.4005898702246\nEpoch: 87 Number of updates: 1\nEpoch: 88 Loss: 211.827832283733\nEpoch: 88 Number of updates: 1\nEpoch: 89 Loss: 206.39792448038304\nEpoch: 89 Number of updates: 1\nEpoch: 90 Loss: 201.107204707402\nEpoch: 90 Number of updates: 1\nEpoch: 91 Loss: 195.95210507588635\nEpoch: 91 Number of updates: 1\nEpoch: 92 Loss: 190.92914915473412\nEpoch: 92 Number of updates: 1\nEpoch: 93 Loss: 186.03494962625288\nEpoch: 93 Number of updates: 1\nEpoch: 94 Loss: 181.26620600186288\nEpoch: 94 Number of updates: 1\nEpoch: 95 Loss: 176.61970239635556\nEpoch: 95 Number of updates: 1\nEpoch: 96 Loss: 172.09230535920537\nEpoch: 96 Number of updates: 1\nEpoch: 97 Loss: 167.68096176147267\nEpoch: 97 Number of updates: 1\nEpoch: 98 Loss: 163.38269673687344\nEpoch: 98 Number of updates: 1\nEpoch: 99 Loss: 159.19461167562605\nEpoch: 99 Number of updates: 1\nEpoch: 100 Loss: 155.11388226972383\nEpoch: 100 Number of updates: 1\nEpoch: 101 Loss: 151.13775660831345\nEpoch: 101 Number of updates: 1\nEpoch: 102 Loss: 147.26355332189618\nEpoch: 102 Number of updates: 1\nEpoch: 103 Loss: 143.48865977409955\nEpoch: 103 Number of updates: 1\nEpoch: 104 Loss: 139.81053029980072\nEpoch: 104 Number of updates: 1\nEpoch: 105 Loss: 136.22668448841307\nEpoch: 105 Number of updates: 1\nEpoch: 106 Loss: 132.7347055111778\nEpoch: 106 Number of updates: 1\nEpoch: 107 Loss: 129.3322384913335\nEpoch: 107 Number of updates: 1\nEpoch: 108 Loss: 126.01698891606442\nEpoch: 108 Number of updates: 1\nEpoch: 109 Loss: 122.78672108915548\nEpoch: 109 Number of updates: 1\nEpoch: 110 Loss: 119.63925662331171\nEpoch: 110 Number of updates: 1\nEpoch: 111 Loss: 116.57247297112495\nEpoch: 111 Number of updates: 1\nEpoch: 112 Loss: 113.58430199369705\nEpoch: 112 Number of updates: 1\nEpoch: 113 Loss: 110.67272856595449\nEpoch: 113 Number of updates: 1\nEpoch: 114 Loss: 107.83578921771347\nEpoch: 114 Number of updates: 1\nEpoch: 115 Loss: 105.07157080958017\nEpoch: 115 Number of updates: 1\nEpoch: 116 Loss: 102.37820924279148\nEpoch: 116 Number of updates: 1\nEpoch: 117 Loss: 99.75388820212748\nEpoch: 117 Number of updates: 1\nEpoch: 118 Loss: 97.1968379310484\nEpoch: 118 Number of updates: 1\nEpoch: 119 Loss: 94.70533403822739\nEpoch: 119 Number of updates: 1\nEpoch: 120 Loss: 92.27769633467845\nEpoch: 120 Number of updates: 1\nEpoch: 121 Loss: 89.9122877006908\nEpoch: 121 Number of updates: 1\nEpoch: 122 Loss: 87.60751298180922\nEpoch: 122 Number of updates: 1\nEpoch: 123 Loss: 85.3618179131139\nEpoch: 123 Number of updates: 1\nEpoch: 124 Loss: 83.17368807107442\nEpoch: 124 Number of updates: 1\nEpoch: 125 Loss: 81.04164785227266\nEpoch: 125 Number of updates: 1\nEpoch: 126 Loss: 78.96425947830306\nEpoch: 126 Number of updates: 1\nEpoch: 127 Loss: 76.94012202618265\nEpoch: 127 Number of updates: 1\nEpoch: 128 Loss: 74.96787048361354\nEpoch: 128 Number of updates: 1\nEpoch: 129 Loss: 73.04617482846348\nEpoch: 129 Number of updates: 1\nEpoch: 130 Loss: 71.17373913184231\nEpoch: 130 Number of updates: 1\nEpoch: 131 Loss: 69.34930068416969\nEpoch: 131 Number of updates: 1\nEpoch: 132 Loss: 67.57162914364497\nEpoch: 132 Number of updates: 1\nEpoch: 133 Loss: 65.83952570654475\nEpoch: 133 Number of updates: 1\nEpoch: 134 Loss: 64.15182229878873\nEpoch: 134 Number of updates: 1\nEpoch: 135 Loss: 62.50738078822871\nEpoch: 135 Number of updates: 1\nEpoch: 136 Loss: 60.90509221712936\nEpoch: 136 Number of updates: 1\nEpoch: 137 Loss: 59.34387605432329\nEpoch: 137 Number of updates: 1\nEpoch: 138 Loss: 57.82267946653596\nEpoch: 138 Number of updates: 1\nEpoch: 139 Loss: 56.34047660838936\nEpoch: 139 Number of updates: 1\nEpoch: 140 Loss: 54.89626793060529\nEpoch: 140 Number of updates: 1\nEpoch: 141 Loss: 53.48907950594201\nEpoch: 141 Number of updates: 1\nEpoch: 142 Loss: 52.11796237240927\nEpoch: 142 Number of updates: 1\nEpoch: 143 Loss: 50.781991893319514\nEpoch: 143 Number of updates: 1\nEpoch: 144 Loss: 49.48026713374285\nEpoch: 144 Number of updates: 1\nEpoch: 145 Loss: 48.21191025294602\nEpoch: 145 Number of updates: 1\nEpoch: 146 Loss: 46.9760659124052\nEpoch: 146 Number of updates: 1\nEpoch: 147 Loss: 45.77190069899362\nEpoch: 147 Number of updates: 1\nEpoch: 148 Loss: 44.598602562954916\nEpoch: 148 Number of updates: 1\nEpoch: 149 Loss: 43.45538027028316\nEpoch: 149 Number of updates: 1\nEpoch: 150 Loss: 42.34146286914064\nEpoch: 150 Number of updates: 1\nEpoch: 151 Loss: 41.256099169952854\nEpoch: 151 Number of updates: 1\nEpoch: 152 Loss: 40.19855723883094\nEpoch: 152 Number of updates: 1\nEpoch: 153 Loss: 39.16812390397921\nEpoch: 153 Number of updates: 1\nEpoch: 154 Loss: 38.164104274755424\nEpoch: 154 Number of updates: 1\nEpoch: 155 Loss: 37.185821273059105\nEpoch: 155 Number of updates: 1\nEpoch: 156 Loss: 36.23261517673222\nEpoch: 156 Number of updates: 1\nEpoch: 157 Loss: 35.30384317466409\nEpoch: 157 Number of updates: 1\nEpoch: 158 Loss: 34.39887893330044\nEpoch: 158 Number of updates: 1\nEpoch: 159 Loss: 33.51711217426458\nEpoch: 159 Number of updates: 1\nEpoch: 160 Loss: 32.65794826280549\nEpoch: 160 Number of updates: 1\nEpoch: 161 Loss: 31.820807806795507\nEpoch: 161 Number of updates: 1\nEpoch: 162 Loss: 31.005126266007302\nEpoch: 162 Number of updates: 1\nEpoch: 163 Loss: 30.210353571406248\nEpoch: 163 Number of updates: 1\nEpoch: 164 Loss: 29.435953754201787\nEpoch: 164 Number of updates: 1\nEpoch: 165 Loss: 28.681404584407616\nEpoch: 165 Number of updates: 1\nEpoch: 166 Loss: 27.946197218666793\nEpoch: 166 Number of updates: 1\nEpoch: 167 Loss: 27.22983585710435\nEpoch: 167 Number of updates: 1\nEpoch: 168 Loss: 26.531837408976035\nEpoch: 168 Number of updates: 1\nEpoch: 169 Loss: 25.851731166887678\nEpoch: 169 Number of updates: 1\nEpoch: 170 Loss: 25.189058489365465\nEpoch: 170 Number of updates: 1\nEpoch: 171 Loss: 24.54337249156304\nEpoch: 171 Number of updates: 1\nEpoch: 172 Loss: 23.91423774389704\nEpoch: 172 Number of updates: 1\nEpoch: 173 Loss: 23.301229978407452\nEpoch: 173 Number of updates: 1\nEpoch: 174 Loss: 22.703935802645255\nEpoch: 174 Number of updates: 1\nEpoch: 175 Loss: 22.121952420893926\nEpoch: 175 Number of updates: 1\nEpoch: 176 Loss: 21.554887362537222\nEpoch: 176 Number of updates: 1\nEpoch: 177 Loss: 21.002358217389773\nEpoch: 177 Number of updates: 1\nEpoch: 178 Loss: 20.463992377812076\nEpoch: 178 Number of updates: 1\nEpoch: 179 Loss: 19.93942678743611\nEpoch: 179 Number of updates: 1\nEpoch: 180 Loss: 19.42830769633196\nEpoch: 180 Number of updates: 1\nEpoch: 181 Loss: 18.930290422450334\nEpoch: 181 Number of updates: 1\nEpoch: 182 Loss: 18.445039119180322\nEpoch: 182 Number of updates: 1\nEpoch: 183 Loss: 17.97222654886529\nEpoch: 183 Number of updates: 1\nEpoch: 184 Loss: 17.511533862124562\nEpoch: 184 Number of updates: 1\nEpoch: 185 Loss: 17.062650382831748\nEpoch: 185 Number of updates: 1\nEpoch: 186 Loss: 16.625273398604897\nEpoch: 186 Number of updates: 1\nEpoch: 187 Loss: 16.199107956667145\nEpoch: 187 Number of updates: 1\nEpoch: 188 Loss: 15.783866664940208\nEpoch: 188 Number of updates: 1\nEpoch: 189 Loss: 15.379269498236468\nEpoch: 189 Number of updates: 1\nEpoch: 190 Loss: 14.985043609419165\nEpoch: 190 Number of updates: 1\nEpoch: 191 Loss: 14.600923145403197\nEpoch: 191 Number of updates: 1\nEpoch: 192 Loss: 14.226649067872417\nEpoch: 192 Number of updates: 1\nEpoch: 193 Loss: 13.8619689785927\nEpoch: 193 Number of updates: 1\nEpoch: 194 Loss: 13.506636949202734\nEpoch: 194 Number of updates: 1\nEpoch: 195 Loss: 13.160413355368016\nEpoch: 195 Number of updates: 1\nEpoch: 196 Loss: 12.823064715185987\nEpoch: 196 Number of updates: 1\nEpoch: 197 Loss: 12.494363531733448\nEpoch: 197 Number of updates: 1\nEpoch: 198 Loss: 12.174088139650035\nEpoch: 198 Number of updates: 1\nEpoch: 199 Loss: 11.862022555654388\nEpoch: 199 Number of updates: 1\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 381.65 248.518125\" width=\"381.65pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M -0 248.518125 \nL 381.65 248.518125 \nL 381.65 0 \nL -0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 39.65 224.64 \nL 374.45 224.64 \nL 374.45 7.2 \nL 39.65 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m334c7e9eba\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"54.868182\" xlink:href=\"#m334c7e9eba\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(51.686932 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"93.10482\" xlink:href=\"#m334c7e9eba\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 25 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(86.74232 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"131.341457\" xlink:href=\"#m334c7e9eba\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 50 -->\n      <g transform=\"translate(124.978957 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"169.578095\" xlink:href=\"#m334c7e9eba\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 75 -->\n      <defs>\n       <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n      </defs>\n      <g transform=\"translate(163.215595 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"207.814733\" xlink:href=\"#m334c7e9eba\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(198.270983 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"246.05137\" xlink:href=\"#m334c7e9eba\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 125 -->\n      <g transform=\"translate(236.50762 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"284.288008\" xlink:href=\"#m334c7e9eba\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 150 -->\n      <g transform=\"translate(274.744258 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"322.524646\" xlink:href=\"#m334c7e9eba\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 175 -->\n      <g transform=\"translate(312.980896 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"360.761284\" xlink:href=\"#m334c7e9eba\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 200 -->\n      <g transform=\"translate(351.217534 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_10\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mac8d3486bd\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#mac8d3486bd\" y=\"215.8892\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0 -->\n      <g transform=\"translate(26.2875 219.688418)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#mac8d3486bd\" y=\"168.138663\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 500 -->\n      <g transform=\"translate(13.5625 171.937882)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#mac8d3486bd\" y=\"120.388127\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 1000 -->\n      <g transform=\"translate(7.2 124.187346)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#mac8d3486bd\" y=\"72.637591\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 1500 -->\n      <g transform=\"translate(7.2 76.43681)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#mac8d3486bd\" y=\"24.887055\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 2000 -->\n      <g transform=\"translate(7.2 28.686274)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#pff36d32675)\" d=\"M 54.868182 17.083636 \nL 57.927113 27.145206 \nL 60.986044 36.697558 \nL 65.57444 50.127321 \nL 70.162837 62.550572 \nL 74.751233 74.042746 \nL 79.33963 84.673623 \nL 83.928026 94.507754 \nL 88.516423 103.604853 \nL 93.10482 112.020157 \nL 97.693216 119.804765 \nL 102.281613 127.005944 \nL 106.870009 133.667422 \nL 111.458406 139.829646 \nL 116.046802 145.530033 \nL 120.635199 150.803197 \nL 125.223595 155.681157 \nL 129.811992 160.193531 \nL 134.400388 164.367719 \nL 138.988785 168.229067 \nL 143.577181 171.801021 \nL 148.165578 175.10527 \nL 152.753974 178.161878 \nL 157.342371 180.989403 \nL 161.930767 183.605017 \nL 168.04863 186.790136 \nL 174.166492 189.661015 \nL 180.284354 192.248657 \nL 186.402216 194.581005 \nL 192.520078 196.683246 \nL 198.63794 198.578083 \nL 206.285267 200.685943 \nL 213.932595 202.537144 \nL 221.579922 204.162936 \nL 230.756715 205.854752 \nL 239.933508 207.30248 \nL 249.110302 208.541335 \nL 259.81656 209.762632 \nL 272.052284 210.911878 \nL 285.817474 211.949198 \nL 301.112129 212.850278 \nL 317.936249 213.605364 \nL 337.819301 214.259698 \nL 359.231818 214.756364 \nL 359.231818 214.756364 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 39.65 224.64 \nL 39.65 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 374.45 224.64 \nL 374.45 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 39.65 224.64 \nL 374.45 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 39.65 7.2 \nL 374.45 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pff36d32675\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"39.65\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "# Number of neurons in each layer\n",
    "neurons = [64, 256, 128, 1]\n",
    "activations = ['sigmoid', 'sigmoid', 'relu']\n",
    "loss_function = 'squared'\n",
    "epochs = 200\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Data preprocessing\n",
    "onehotencoded = False\n",
    "minibatch = True\n",
    "batch_size = 1500\n",
    "X, Y = preprocess()\n",
    "# Y = onehotencoding(Y)\n",
    "X, Y = create_minibatches(X, Y, batch_size)\n",
    "\n",
    "# Split training and test data\n",
    "num_training = 1500 // batch_size\n",
    "train_X, train_Y = X[: num_training], Y[: num_training]\n",
    "test_X, test_Y = X[num_training:], Y[num_training:]\n",
    "\n",
    "print(train_X.shape, train_Y.shape)\n",
    "\n",
    " # Train the network\n",
    "neuralnet = Neural_Network(neurons, activations, epochs, learning_rate, loss_function)\n",
    "loss = neuralnet.train(train_X, train_Y, minibatch=minibatch)\n",
    "\n",
    "# Plot the results\n",
    "plt.plot(loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}