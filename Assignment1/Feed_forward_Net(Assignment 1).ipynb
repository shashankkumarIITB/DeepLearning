{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network():\n",
    "    def __init__(self, neurons, Activations): \n",
    "        # arguments: an array \"neurons\" consist of number of neurons for each layer, \n",
    "        # an array \"activations\" consisting of activation functions used for the hidden layers and output layer\n",
    "        self.inputSize = neurons[0] # Number of neurons in input layer\n",
    "        self.outputSize = neurons[-1] # Number of neurons in output layer\n",
    "        self.layers = len(neurons)\n",
    "        self.weights = [] #weights for each layer\n",
    "        self.biases = [] #biases in each layer \n",
    "        self.layer_activations = [] #activations in each layer\n",
    "        for i in range(len(neurons)-1): \n",
    "            self.weights.append(np.random.rand(neurons[i+1],neurons[i])) #weight matrix between layer i and layer i+1\n",
    "            self.biases.append(np.random.rand(neurons[i+1],1))\n",
    "            self.layer_activations.append(Activations[i]) #activations for each layer\n",
    "        \n",
    "            \n",
    "    def sigmoid(self, z): # sigmoid activation function\n",
    "        #Fill in the details to compute and return the sigmoid activation function                  \n",
    "        return 1.0/(1.0+np.exp(-z))\n",
    "    \n",
    "    def sigmoidPrime(self,z): # derivative of sigmoid activation function\n",
    "        #Fill in the details to compute and return the derivative of sigmoid activation function\n",
    "        return self.sigmoid(z)*(1-self.sigmoid(z))\n",
    "\n",
    "                          \n",
    "    def tanh(self, z): # hyperbolic tan activation function\n",
    "        #Fill in the details to compute and return the tanh activation function                  \n",
    "        pass\n",
    "    \n",
    "    def tanhPrime(self,x): # derivative of hyperbolic tan activation function\n",
    "        #Fill in the details to compute and return the derivative of tanh activation function\n",
    "        pass\n",
    "                          \n",
    "    def linear(self, z): # Linear activation function\n",
    "        #Fill in the details to compute and return the linear activation function                                    \n",
    "        pass\n",
    "    \n",
    "    def linearPrime(self,x): # derivative of linear activation function\n",
    "        #Fill in the details to compute and return the derivative of activation function                                                      \n",
    "        pass\n",
    "\n",
    "    def ReLU(self,z): # ReLU activation function\n",
    "        #Fill in the details to compute and return the ReLU activation function                  \n",
    "        pass\n",
    "    \n",
    "    def ReLUPrime(self,z): # derivative of ReLU activation function\n",
    "        #Fill in the details to compute and return the derivative of ReLU activation function\n",
    "        pass\n",
    "    \n",
    "    def forward(self, a): # function of forward pass which will receive input and give the output of final layer\n",
    "        # Write the forward pass using the weights and biases to find the predicted value and return them.\n",
    "        layer_activations_a = [a] #store the input as the input layer activations\n",
    "        layer_dot_prod_z = []\n",
    "        for i, param in enumerate(zip(self.biases, self.weights)):\n",
    "            b, w = param[0], param[1]\n",
    "            if self.layer_activations[i].lower()=='sigmoid':\n",
    "                z = np.dot(w, a)+b\n",
    "                a = self.sigmoid(z)\n",
    "            elif self.layer_activations[i].lower()=='relu':\n",
    "                pass\n",
    "            elif self.layer_activations[i].lower()=='tanh':   \n",
    "                pass\n",
    "            elif self.layer_activations[i].lower()=='linear':\n",
    "                pass\n",
    "            layer_dot_prod_z.append(z)    \n",
    "            layer_activations_a.append(a)\n",
    "        return a, layer_dot_prod_z, layer_activations_a\n",
    "                          \n",
    "            \n",
    "    \n",
    "    def backward(self, x, y, zs, activations): # find the loss and return derivative of loss w.r.t every parameter\n",
    "        # Write the backpropagation algorithm here to find the gradients of weights and biases and return them.\n",
    "        grad_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        grad_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        # backward pass\n",
    "        if self.layer_activations[-1].lower()=='sigmoid':\n",
    "            delta = (activations[-1] - y) * \\\n",
    "                self.sigmoidPrime(zs[-1])\n",
    "        elif self.layer_activations[-1].lower()=='relu':\n",
    "            pass\n",
    "        elif self.layer_activations[-1].lower()=='tanh':   \n",
    "            pass\n",
    "        elif self.layer_activations[-1].lower()=='linear':\n",
    "            pass\n",
    "\n",
    "        # fill in the appropriate details for gradients of w and b\n",
    "        #grad_b[-1] = \n",
    "        #grad_w[-1] = \n",
    "                 \n",
    "        for l in range(2, self.layers): # Here l is in backward sense i.e. last l th layer\n",
    "            z = zs[-l]\n",
    "            if self.layer_activations[-l].lower()=='sigmoid':\n",
    "                prime = self.sigmoidPrime(z)\n",
    "            elif self.layer_activations[-l].lower()=='relu':\n",
    "                pass\n",
    "            elif self.layer_activations[-l].lower()=='tanh':   \n",
    "                pass\n",
    "            elif self.layer_activations[-l].lower()=='linear':\n",
    "                pass\n",
    "\n",
    "            #Compute delta, gradients of b and w \n",
    "            #delta = \n",
    "            #grad_b[-l] = \n",
    "            #grad_w[-l] = \n",
    "                          \n",
    "        return (grad_b, grad_w)                 \n",
    "\n",
    "    def update_parameters(self, grads, learning_rate): # update the parameters using the gradients\n",
    "        # update weights and biases using the gradients and the learning rate\n",
    "        \n",
    "        grad_b, grad_w = grads[0], grads[1]       \n",
    "        \n",
    "        #Implement the update rule for weights  and biases\n",
    "        #self.weights = \n",
    "        #self.biases = \n",
    "        \n",
    "    def loss(self, predicted, actual):\n",
    "        #Implement the loss function\n",
    "        return 0.5*(predicted-actual)**2\n",
    "                     \n",
    "    def train(self, X, Y, minibatch=False): # receive the full training data set\n",
    "        lr = 1e-3         # learning rate\n",
    "        epochs = 1000     # number of epochs\n",
    "        loss_list = []\n",
    "        if minibatch==False:\n",
    "            for e in range(epochs): \n",
    "                losses = []\n",
    "                for q in range(len(X)):\n",
    "                    train_x = np.resize(X[q],(X[q].shape[0],1)) \n",
    "                    if not onehotencoded: \n",
    "                        train_y = np.resize(Y[q],(1,1)) \n",
    "                    else:\n",
    "                        train_y = np.resize(np.argmax(Y[q]),(1,1)) \n",
    "                    out, dot_prod_z, activations_a = self.forward(train_x)\n",
    "                    loss = self.loss(out, train_y)\n",
    "                    grads = self.backward(train_x, train_y, dot_prod_z, activations_a) # find the gradients using backward pass\n",
    "                    self.update_parameters(grads, lr)\n",
    "                    losses.append(loss)\n",
    "                loss_list.append(np.mean(np.array(losses)))\n",
    "                print(f'Epoch: {e} Loss: {np.mean(np.array(losses))}')\n",
    "        else:\n",
    "            minibatchsize = 20\n",
    "            create_minibatches(X,Y,minibatchsize)\n",
    "            \n",
    "            for e in range(epochs):\n",
    "                #Complete the training code with minibatches \n",
    "                pass\n",
    "        return loss_list\n",
    "        \n",
    "    def predict(self, x):\n",
    "        print (\"Input : \\n\" + str(x))\n",
    "        prediction,_,_ = self.forward(x)\n",
    "        print (\"Output: \\n\" + str(prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a method for creating one hot encoded labels \n",
    "def onehotencoding(Y):\n",
    "    pass\n",
    "\n",
    "#a method to create minibatches \n",
    "def create_minibatches(X,Y,minibatchsize):\n",
    "    numbatches = int(np.ceil(len(X)/minibatchsize))\n",
    "    idx = np.arange(len(X))\n",
    "    np.random.shuffle(idx)\n",
    "    X_minibatches = []\n",
    "    Y_minibatches = [] \n",
    "    for i in range(numbatches):\n",
    "        idx_minibatch = idx[i*minibatchsize:min(len(idx),(i+1)*minibatchsize)]\n",
    "        xn = np.take(X,idx_minibatch,axis=0) \n",
    "        yn = np.take(Y,idx_minibatch,axis=0)\n",
    "        X_minibatches.append(xn)\n",
    "        Y_minibatches.append(yn)\n",
    "    return X_minibatches, Y_minibatches\n",
    "\n",
    "def test_create_minibatches():\n",
    "    X = []\n",
    "    Y = []\n",
    "    inputsize = 3\n",
    "    minibatch = False\n",
    "    onehotencoded = False\n",
    "    n_batch = 20\n",
    "    batch_size = 5\n",
    "    for i in range(50):\n",
    "        if(i % 2 == 0):\n",
    "            X.append([np.random.randint(1,10) for i1 in range(inputsize)])\n",
    "            Y.append(1)\n",
    "        else:\n",
    "            X.append([np.random.randint(-10,1) for i1 in range(inputsize)])\n",
    "            Y.append(0)\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    X_mb, Y_mb = create_minibatches(X,Y,6)\n",
    "    print(X_mb, Y_mb)\n",
    "\n",
    "#test_create_minibatches()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating some training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "inputsize = 3\n",
    "minibatch = False\n",
    "onehotencoded = False\n",
    "n_batch = 20\n",
    "batch_size = 5\n",
    "for i in range(500):\n",
    "    if(i % 2 == 0):\n",
    "        X.append([random.randint(1,10) for i1 in range(inputsize)])\n",
    "        Y.append(1)\n",
    "    else:\n",
    "        X.append([random.randint(-10,1) for i1 in range(inputsize)])\n",
    "        Y.append(0)\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "if onehotencoded:\n",
    "    Y = onehotencoding(Y)\n",
    "\n",
    "if minibatch==False:\n",
    "    train_X = X\n",
    "    train_Y = Y\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(train_X.shape, train_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#D_in is input dimension\n",
    "#H1 is dimension of first hidden layer \n",
    "#H2 is dimension of second hidden layer\n",
    "#D_out is output dimension.\n",
    "D_in, H1, H2, D_out = inputsize, 10, 5, 1 #You can add more layers if you wish to \n",
    "\n",
    "neurons = [D_in, H1, H2, D_out] # list of number of neurons in the layers sequentially.\n",
    "activation_functions = ['sigmoid','sigmoid', 'sigmoid'] #activations in each layer (Note: the input layer does not have any activation)\n",
    "my_neuralnet = Neural_Network(neurons, activation_functions )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss = my_neuralnet.train(train_X,train_Y,minibatch=minibatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction for a data point after the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_neuralnet.predict(np.array([8,4,9]).reshape((3,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
