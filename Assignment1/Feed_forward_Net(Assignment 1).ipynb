{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Neural Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     1,
     19,
     23,
     27,
     33,
     37,
     41,
     45,
     49,
     53,
     58,
     80,
     93,
     100,
     140,
     148,
     177
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Neural_Network():\n",
    "    def __init__(self, neurons, Activations, epochs=1000, learning_rate=0.001, loss='squared'): \n",
    "        # arguments: an array \"neurons\" consist of number of neurons for each layer, \n",
    "        # an array \"activations\" consisting of activation functions used for the hidden layers and output layer\n",
    "        self.inputSize = neurons[0] # Number of neurons in input layer\n",
    "        self.outputSize = neurons[-1] # Number of neurons in output layer\n",
    "        self.layers = len(neurons)\n",
    "        self.weights = [] #weights for each layer\n",
    "        self.biases = [] #biases in each layer \n",
    "        self.layer_activations = [] #activations in each layer\n",
    "        self.epochs = epochs #number of epochs to train the network\n",
    "        self.learning_rate = learning_rate #learning rate used for training\n",
    "        self.loss_function = loss\n",
    "        np.random.seed(0)\n",
    "        for i in range(len(neurons)-1): \n",
    "            self.weights.append(np.random.rand(neurons[i+1],neurons[i])) #weight matrix between layer i and layer i+1\n",
    "            self.biases.append(np.random.rand(neurons[i+1],1))\n",
    "            self.layer_activations.append(Activations[i]) #activations for each layer\n",
    "        \n",
    "    def sigmoid(self, z): # sigmoid activation function\n",
    "        #Fill in the details to compute and return the sigmoid activation function                  \n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    def sigmoidPrime(self, z): # derivative of sigmoid activation function\n",
    "        #Fill in the details to compute and return the derivative of sigmoid activation function\n",
    "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
    "                          \n",
    "    def tanh(self, z): # hyperbolic tan activation function\n",
    "        #Fill in the details to compute and return the tanh activation function                  \n",
    "        exp_z = np.exp(z)\n",
    "        exp_z_negative = np.exp(-z)\n",
    "        return (exp_z - exp_z_negative) / (exp_z + exp_z_negative)  \n",
    "    \n",
    "    def tanhPrime(self, z): # derivative of hyperbolic tan activation function\n",
    "        #Fill in the details to compute and return the derivative of tanh activation function\n",
    "        return 1 - self.tanh(z) ** 2\n",
    "                          \n",
    "    def linear(self, z): # Linear activation function\n",
    "        #Fill in the details to compute and return the linear activation function                                    \n",
    "        return z\n",
    "    \n",
    "    def linearPrime(self, z): # derivative of linear activation function\n",
    "        #Fill in the details to compute and return the derivative of activation function                                                      \n",
    "        return np.ones(z.shape)\n",
    "\n",
    "    def ReLU(self, z): # ReLU activation function\n",
    "        #Fill in the details to compute and return the ReLU activation function                  \n",
    "        return np.where(z < 0, 0, z)\n",
    "    \n",
    "    def ReLUPrime(self, z): # derivative of ReLU activation function\n",
    "        #Fill in the details to compute and return the derivative of ReLU activation function\n",
    "        return np.where(z < 0, 0, 1)\n",
    "    \n",
    "    def softmax(self, z): # Softmax activation function\n",
    "        z = z - np.max(z, axis=0)\n",
    "        z_exp = np.exp(z)\n",
    "        return z_exp / np.sum(z_exp, axis=0)          \n",
    "    \n",
    "    def forward(self, a): # function of forward pass which will receive input and give the output of final layer\n",
    "        # Write the forward pass using the weights and biases to find the predicted value and return them.\n",
    "        layer_activations_a = [a] #store the input as the input layer activations\n",
    "        layer_dot_prod_z = []\n",
    "        for i, param in enumerate(zip(self.biases, self.weights)):\n",
    "            b, w = param[0], param[1]\n",
    "            z = np.dot(w, a) + b\n",
    "            if self.layer_activations[i].lower()  == 'sigmoid':\n",
    "                a = self.sigmoid(z)\n",
    "            elif self.layer_activations[i].lower() == 'relu':\n",
    "                a = self.ReLU(z)    \n",
    "            elif self.layer_activations[i].lower() == 'tanh':   \n",
    "                a = self.tanh(z)\n",
    "            elif self.layer_activations[i].lower() == 'linear':\n",
    "                a = self.linear(z)\n",
    "            elif self.layer_activations[i].lower() == 'softmax':\n",
    "                a = self.softmax(z)\n",
    "            layer_dot_prod_z.append(z)\n",
    "            layer_activations_a.append(a)\n",
    "\n",
    "        return a, layer_dot_prod_z, layer_activations_a\n",
    "        \n",
    "    def loss(self, Y_hat, Y):\n",
    "        #Implement the loss function\n",
    "        epsilon = 10 ** -5\n",
    "#         print(f'Y : {Y.shape}')\n",
    "#         print(f'Y_hat : {Y_hat.shape}')\n",
    "        if self.loss_function.lower() == 'cross-entropy':\n",
    "            Y_hat = np.where(Y_hat == 1, 1 - epsilon, Y_hat)\n",
    "            Y_hat = np.where(Y_hat == 0, epsilon, Y_hat)\n",
    "            error = - Y * np.log(Y_hat) - (1 - Y) * np.log(1 - Y_hat) \n",
    "        elif self.loss_function.lower() == 'squared':\n",
    "            error = 0.5 * (Y_hat - Y) ** 2\n",
    "        return error\n",
    "\n",
    "    def loss_grad(self, Y_hat, Y):\n",
    "        #Return the gradient of the loss function -\n",
    "        # 1. w.r.t. activations for squared-error \n",
    "        # 2. w.r.t. z for cross-entropy loss\n",
    "        grad = Y_hat - Y\n",
    "        return grad\n",
    "        \n",
    "    def backward(self, x, y, zs, activations): # find the loss and return derivative of loss w.r.t every parameter\n",
    "        # Write the backpropagation algorithm here to find the gradients of weights and biases and return them.\n",
    "        # Assuming L2 loss\n",
    "        grad_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        grad_w = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "        delta = self.loss_grad(activations[-1], y)\n",
    "        if self.layer_activations[-1].lower() == 'sigmoid':\n",
    "            delta = delta * self.sigmoidPrime(zs[-1])\n",
    "        elif self.layer_activations[-1].lower() == 'relu':\n",
    "            delta = delta * self.ReLUPrime(zs[-1])\n",
    "        elif self.layer_activations[-1].lower() == 'tanh':\n",
    "            delta = delta * self.tanhPrime(zs[-1])   \n",
    "        elif self.layer_activations[-1].lower() == 'linear':\n",
    "            delta = delta * self.linearPrime(zs[-1])\n",
    "        \n",
    "        # Number of training examples = m\n",
    "        m = delta.shape[1]\n",
    "        # fill in the appropriate details for gradients of w and b\n",
    "        grad_w[-1] = np.dot(delta, activations[-2].T) / m \n",
    "        grad_b[-1] = np.sum(delta, axis=1, keepdims=True) / m\n",
    "\n",
    "        for l in range(2, self.layers): # Here l is in backward sense i.e. last lth layer\n",
    "            z = zs[-l]\n",
    "            if self.layer_activations[-l].lower() == 'sigmoid':\n",
    "                prime = self.sigmoidPrime(z)\n",
    "            elif self.layer_activations[-l].lower() == 'relu':\n",
    "                prime = self.ReLUPrime(z)\n",
    "            elif self.layer_activations[-l].lower() == 'tanh':   \n",
    "                prime = self.tanhPrime(z)\n",
    "            elif self.layer_activations[-l].lower() == 'linear':\n",
    "                prime = self.linearPrime(z)\n",
    "\n",
    "            #Compute delta, gradients of b and w\n",
    "            delta = np.dot(self.weights[-l+1].T, delta) * prime\n",
    "            grad_w[-l] = np.dot(delta, activations[-l-1].T) / m \n",
    "            grad_b[-l] = np.sum(delta, axis=1, keepdims=True) / m\n",
    "\n",
    "        return (grad_b, grad_w)   \n",
    "\n",
    "    def update_parameters(self, grads, learning_rate): # update the parameters using the gradients\n",
    "        # update weights and biases using the gradients and the learning rate\n",
    "        grad_b, grad_w = grads[0], grads[1]       \n",
    "        \n",
    "        #Implement the update rule for weights  and biases\n",
    "        self.weights = [self.weights[i] - learning_rate * grad_w[i] for i in range(len(self.weights))]\n",
    "        self.biases = [self.biases[i] - learning_rate * grad_b[i] for i in range(len(self.biases))] \n",
    "        \n",
    "    def train(self, X, Y, minibatch=False, batch_size=20, verbose=False): # receive the full training data set\n",
    "        lr = self.learning_rate # learning rate\n",
    "        epochs = self.epochs # number of epochs\n",
    "        loss_list = []\n",
    "        for e in range(epochs): \n",
    "            losses = []\n",
    "            for q in range(len(X)):\n",
    "                if minibatch == False:\n",
    "                    rows_x, cols_x = X[q].shape[0], 1\n",
    "                    rows_y, cols_y = Y[q].shape[0], 1\n",
    "                else:\n",
    "                    rows_x, cols_x = X[q].shape[1], X[q].shape[0]\n",
    "                    rows_y, cols_y = Y[q].shape[1], Y[q].shape[0]\n",
    "\n",
    "                train_x = np.resize(X[q], (rows_x, cols_x))\n",
    "                train_y = np.resize(Y[q],(rows_y, cols_y))\n",
    "                \n",
    "                out, dot_prod_z, activations_a = self.forward(train_x)\n",
    "                loss = self.loss(out, train_y)\n",
    "                grads = self.backward(train_x, train_y, dot_prod_z, activations_a) # find the gradients using backward pass\n",
    "                self.update_parameters(grads, lr)\n",
    "                losses.append(loss)\n",
    "            \n",
    "            loss_mean = np.mean(np.array(losses))\n",
    "            loss_list.append(loss_mean)\n",
    "            if verbose:\n",
    "                print(f'Epoch: {e} Loss: {loss_mean}')\n",
    "        return loss_list\n",
    "        \n",
    "    def predict(self, x):\n",
    "        print (\"Input : \\n\" + str(x))\n",
    "        prediction,_,_ = self.forward(x)\n",
    "        print (\"Output: \\n\" + str(prediction))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Miscellaneous Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### One-hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# a method for creating one hot encoded labels \n",
    "def onehotencoding(Y):\n",
    "    rows = Y.shape[0]\n",
    "    values = {e:i for i, e in enumerate(np.unique(Y))}\n",
    "    y_enc = np.zeros((rows, len(values)))\n",
    "    for i in range(rows):\n",
    "        j = values[Y[i]]\n",
    "        y_enc[i][j] = 1\n",
    "    return y_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Create mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#a method to create minibatches \n",
    "def create_minibatches(X, Y, minibatchsize):\n",
    "    numbatches = int(np.ceil(len(X)/minibatchsize))\n",
    "    idx = np.arange(len(X))\n",
    "    np.random.shuffle(idx)\n",
    "    X_minibatches = []\n",
    "    Y_minibatches = [] \n",
    "    for i in range(numbatches):\n",
    "        idx_minibatch = idx[i*minibatchsize:min(len(idx),(i+1)*minibatchsize)]\n",
    "        xn = np.take(X,idx_minibatch,axis=0) \n",
    "        yn = np.take(Y,idx_minibatch,axis=0)\n",
    "        X_minibatches.append(xn)\n",
    "        Y_minibatches.append(yn)\n",
    "    X_minibatches, Y_minibatches = np.array(X_minibatches), np.array(Y_minibatches)\n",
    "    return X_minibatches, Y_minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Test mini-batches created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_create_minibatches():\n",
    "    X = []\n",
    "    Y = []\n",
    "    batch_size = 2\n",
    "    for i in range(20):\n",
    "        if(i % 2 == 0):\n",
    "            X.append([np.random.randint(1,10) for i1 in range(inputsize)])\n",
    "            Y.append(1)\n",
    "        else:\n",
    "            X.append([np.random.randint(-10,1) for i1 in range(inputsize)])\n",
    "            Y.append(0)\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    X_mb, Y_mb = create_minibatches(X,Y,batch_size)\n",
    "    print(X_mb, Y_mb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Generate random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def generateData(inputsize=3, onehotencoded=False, minibatch=False, batch_size=5):\n",
    "    X = []\n",
    "    Y = []\n",
    "    random.seed(0)\n",
    "    for i in range(500):\n",
    "        if(i % 2 == 0):\n",
    "            X.append([random.randint(1,10) for i1 in range(inputsize)])\n",
    "            Y.append(1)\n",
    "        else:\n",
    "            X.append([random.randint(-10,1) for i1 in range(inputsize)])\n",
    "            Y.append(0)\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    if onehotencoded:\n",
    "        Y = onehotencoding(Y)\n",
    "\n",
    "    if minibatch == False:\n",
    "        train_X = X\n",
    "        train_Y = Y\n",
    "    else:\n",
    "        train_X, train_Y = create_minibatches(X, Y, batch_size)\n",
    "    return train_X, train_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2(1)(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 3) (500, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3de5xddXnv8c93z30yycxkcoEkQMJNDHKzAcQLKlbEVqCtWIlWqaKoLWprjxZOexQ5p6dHraJWtOKlVWxFRS1REbTgvYgkiIQAgSEkJNwScr/PZT/nj7V2smfYM7NmMnv2zN7f9+u1Xnut37rsZ7F5zZPf+q31LEUEZmZmg+UqHYCZmU1OThBmZlaSE4SZmZXkBGFmZiU5QZiZWUn1lQ5gvMyaNSsWLlxY6TDMzKaUFStWPBMRs0utq5oEsXDhQpYvX17pMMzMphRJ64Za50tMZmZWkhOEmZmV5ARhZmYlOUGYmVlJThBmZlaSE4SZmZXkBGFmZiXVfILYua+Xa378EPes31bpUMzMJpWaTxD5PHzqtodZsW5rpUMxM5tUaj5BTG+uR4Lte3oqHYqZ2aRS8wkilxPtLQ1s29tb6VDMzCaVsiYISedJWi2pW9IVJdafLeluSX2SLipqP1XSHZJWSbpX0uvLGWdHSwNb9zhBmJkVK1uCkFQHXAu8GlgMLJW0eNBmjwF/DvzHoPY9wJsj4kTgPOCTkjrKFWtHayPbfInJzGyAclZzPQPojog1AJJuAC4E7i9sEBFr03X54h0j4qGi+SckbQRmA2W51aijtYEtu50gzMyKlfMS03xgfdHyhrRtVCSdATQCj5RYd5mk5ZKWb9q0acyBdrQ0sM2XmMzMBhixByHpeOD9wFHF20fEOWWMq/DdhwPXA5dERH7w+oi4DrgOYMmSJTHW7+lobWSrLzGZmQ2Q5RLTt4B/Ab4A9I/i2I8DRxQtL0jbMpE0A/gB8HcR8etRfO+odbQ2sHNfH339eerrav7GLjMzIFuC6IuIz43h2HcBx0laRJIYLgbekGVHSY3Ad4GvRsSNY/juUeloaQBgx74+Zk5rLPfXmZlNCUP+c1nSTEkzge9J+gtJhxfa0vZhRUQfcDlwK/AA8M2IWCXpakkXpN9xuqQNwOuAz0tale7+p8DZwJ9LuiedTj20Ux1aR2uSFHwnk5nZQcP1IFYAAShdfn/RugCOHungEXEzcPOgtg8Wzd9Fculp8H5fA7420vHHS3tr0oPwsxBmZgcNmSAiYtFEBlJJnWkPYvte9yDMzApGHJGV9JfFD6lJ6pT0F+UNa2IVxiB8q6uZ2UFZbtl5e0QceEAtIrYCby9fSBOvo9UJwsxssCwJok5SYRyiUEKjqm71md7cgORBajOzYlluc70V+Iakz6fL7wBuKV9IE6/OFV3NzJ4lS4L4AHAZ8K50+cckD81VFZfbMDMbKEuCeHdEfIrkaWoAJL0X+FTZoqqA9tZG9yDMzIpkGYO4pETbn49zHBXX0dLgt8qZmRUZsgchaSlJaYxFkpYVrZoObCl3YBOts7WBR5/ZXekwzMwmjeEuMf038CQwC/h4UftO4N5yBlUJfmmQmdlAwz1JvQ5YB5w1ceFUTntLAzv29dGfD+pyGnkHM7Mql+VJ6hdIukvSLkk9kvol7ZiI4CZS4WG5HR6oNjMDsg1SfwZYCjwMtABvI3nXdFUp1GPyi4PMzBKZ3o4TEd1AXUT0R8S/AueVN6yJV6jo6ltdzcwSWZ6D2JO+wOceSR8lGbiuuteuFQr2bffDcmZmQLY/9G9Kt7sc2E3yGtHXljOoSjjw0iCX/DYzAzL0ICJiXdqDWAh8B1gdEVX3V7Sz8NKg3e5BmJlBhgQh6Q9Jymw8QvJ2uUWS3hERPyx3cBPpQEVXj0GYmQHZxiA+Drw8HahG0jHAD4CqShB1OTGj2eU2zMwKsoxB7Cwkh9Qakqepq05Hq0t+m5kVDFeL6U/S2eWSbga+CQTwOuCuCYhtwnW0NrLVdzGZmQHDX2I6v2j+aeCl6fwmoLlsEVVQ8k4IX2IyM4PhazG9ZSIDmQw6WhtYu9kVXc3MYJQPvEm6u1yBTAadrY1+q5yZWWq0T0SPqsyppPMkrZbULemKEuvPlnS3pD5JFw1ad4mkh9Op1EuLxl1S0bWX/nxMxNeZmU1qo00QP8i6oaQ6kqJ+rwYWA0slLR602WMkb6f7j0H7zgQ+BJwJnAF8SFLnKGMdtY7WBiJc0dXMDEaZICLi70ex+RlAd0SsSZ+8vgG4cNDx1kbEvUB+0L6vAn4cEVsiYivwYyagQGCHC/aZmR0wZIKQ9Mv0c6ekHUXTzozvg5gPrC9a3pC2ZZFpX0mXSVouafmmTZsyHnpoB+ox+U4mM7OhE0REvDj9nB4RM4qm6RExY+JCHFpEXBcRSyJiyezZsw/5eIWKru5BmJllvMQkqU7SPElHFqYMuz1OUvm1YEHalsWh7Dtm7kGYmR2UpVjfu0kGjJ/m4FhBACePsOtdwHGSFpH8cb8YeEPGuG4F/m/RwPS5wJUZ9x2zAz0I3+pqZpapWN97gedExObRHDgi+iRdTvLHvg74ckSsknQ1sDwilkk6Hfgu0AmcL+nDEXFiRGyR9L85WNLj6ojYMprvH4sZLWlFVycIM7NMCWI9sH0sB4+Im4GbB7V9sGj+LpLLR6X2/TLw5bF871gdqOjqMQgzs0wJYg3wU0k/APYXGiPiE2WLqoI6WhvY6jEIM7NMCeKxdGpMp6qWFOxzD8LMLMsrRz88EYFMFh2tjb7N1cyM4d8H8cmI+CtJ3yO5a2mAiLigrJFVSEdrA+tc0dXMbNgexPXp5z9NRCCTRUdLg18aZGbG8O+DWJF+/mziwqm89tbGAxVd63KjKl5rZlZVRnySWtJrJP1W0pZR1mKakjrTiq4797kXYWa1LctdTJ8E/gRYGRFV/6KEAxVd9/QeKL1hZlaLstRiWg/cVwvJAaCjJUkKfhbCzGpdlh7EB4CbJf2MGnhQrt3vhDAzA7IliH8AdgHN1MCDcp3pZaXtvpPJzGpclgQxLyKeV/ZIJomDFV19icnMaluWMYibJZ1b9kgmiRlpgvCzEGZW67IkiHcBt0jaWwu3uSYVXetd0dXMal6WWkzTJyKQyaRzWqMvMZlZzcv0ytFa09HS4LuYzKzmOUGU0N7a6DEIM6t5ThAldLQ0sN2XmMysxmWpxXR9lrZq0tnqS0xmZll6ECcWL0iqA36vPOFMDu2tjWzf20s+XxPVRczMShoyQUi6UtJO4OT09tYd6fJG4KYJi7ACOloKFV37Kh2KmVnFDJkgIuIf01tcPxYRM9JpekR0RcSVExjjhCtUdHXBPjOrZVlKbfxQ0tmDGyPi52WIZ1Io1GPyOISZ1bIsCeL9RfPNwBnACuCcskQ0CbS7B2FmNvIgdUScXzS9EngesDXLwSWdJ2m1pG5JV5RY3yTpG+n6OyUtTNsbJH1F0kpJD0ia0EtaXdOSHsSWXU4QZla7xvIcxAbguSNtlN7tdC3wamAxsFTS4kGbXQpsjYhjgWuAj6TtrwOaIuIkkjum3lFIHhNhZiFB7HaCMLPaNeIlJkn/DBTu98wBpwJ3Zzj2GUB3RKxJj3MDcCFwf9E2FwJXpfM3Ap+RpPT7pkmqB1qAHmDCCgS2NdXTWJ/jmd37R97YzKxKZRmDWF403wd8PSJ+lWG/+SSvKy3YAJw51DYR0SdpO9BFkiwuBJ4EWoG/jogtg79A0mXAZQBHHnlkhpCykUTXtEZfYjKzmpalmutXJDUCx6dNq8sbEpD0PvqBeUAn8AtJ/1XojRTFdh1wHcCSJUvG9am2rrZGNvsSk5nVsCylNl4GPEwynvBZ4KFSt72W8DhwRNHygrSt5Dbp5aR2YDPwBuCWiOiNiI3Ar4AlGb5z3Myc1uQEYWY1Lcsg9ceBcyPipRFxNvAqkgHlkdwFHCdpUdoDuRhYNmibZcAl6fxFwO0REcBjpLfRSpoGvAB4MMN3jptZ0xrZvMtjEGZWu7IkiIaIOHBZKSIeAhpG2iki+oDLgVuBB4BvRsQqSVdLuiDd7EtAl6Ru4H1A4VbYa4E2SatIEs2/RsS9WU9qPMyc1ui7mMyspmUapJb0ReBr6fIbGThwPaSIuBm4eVDbB4vm95Hc0jp4v12l2idSV1sTe3r62dvTT0tjXSVDMTOriKzvpL4feE863Z+2VbXCw3KbfaurmdWoLHcx7Qc+kU41o6stTRC7eljQ2VrhaMzMJl6WB+VeRPIw21HF20fE0eULq/L8NLWZ1bosYxBfAv6apEBff3nDmTxmtTUB8IzvZDKzGpUlQWyPiB+WPZJJZuaBMQj3IMysNg2ZICQ9P539iaSPAd8BDvxzOiKy1GOaslob62huyPkSk5nVrOF6EB8ftFz8JHNQxe+DgEI9piZfYjKzmjVkgoiIlwNIOnpwDSRJVT1AXdDV5oflzKx2ZXkO4sYSbd8a70Amo5nTGtnsiq5mVqOGG4M4ATgRaJf0J0WrZpC8erTqdU1r4uGnd1U6DDOzihhuDOI5wGuADuD8ovadwNvLGdRk0dXWyDO79hMRJO8xMjOrHcONQdwE3CTprIi4YwJjmjS6pjWyvy/Pnp5+pjVluSPYzKx6DHeJ6QMR8VHgDZKWDl4fEe8pa2STQFfRw3JOEGZWa4b7q/dA+pmpcms1mjM9SRAbd+7nqK5pFY7GzGxiDXeJ6Xvp7C8j4pEJimdSmTMjTRA7/CyEmdWeLNdNvixpAcmLe34B/DwiVpY3rMlhzvTkZq2NO/dVOBIzs4mXpdz3S9NXhp4OvAz4gaS2iJhZ7uAqrbO1gYY6sXGnexBmVnuylPt+MfCSdOoAvk/Sk6h6kpjd1sTTO9yDMLPak+US009JSn3/I3BzRNTUo8WzZzSzyT0IM6tBWRLELOBFwNnAeyTlgTsi4n+VNbJJYs70Jh7bvKfSYZiZTbgRazFFxDZgDfAo8CRwDEmyqAlzpjd5kNrMalKWMYg1wIMk4w6fA95SS5eZ5kxvZuueXnr68jTWZ6ltaGZWHbJcYjo2IvJlj2SSmps+C7Fp137md7RUOBozs4mT5RJTzSYHKH5YzpeZzKy2lPWaiaTzJK2W1C3pihLrmyR9I11/p6SFRetOlnSHpFWSVkqqSInxwsNyT/tpajOrMWVLEJLqgGuBVwOLgaWSFg/a7FJga0QcC1wDfCTdtx74GvDOiDiR5AG93nLFOpxCPaZNHqg2sxozYoKQ9F5JM5T4kqS7JZ2b4dhnAN0RsSYd1L4BuHDQNhcCX0nnbwReoeTFC+cC90bE7wAiYnNE9Gc9qfHU1dZETvhpajOrOVl6EG+NiB0kf7Q7gTcB/y/DfvOB9UXLG9K2kttERB+wHegCjgdC0q1pQvpAqS+QdJmk5ZKWb9q0KUNIo1eXE11tTS7YZ2Y1J0uCKLxK7Q+A6yNiVVFbudQDLwbemH7+saRXDN4oIq6LiCURsWT27NllC+bw9mae9CC1mdWYLAlihaQfkSSIWyVNB7Lc2fQ4cETR8oK0reQ26bhDO7CZpLfx84h4JiL2ADcDz8/wnWUxr72FJ7btrdTXm5lVRJYEcSlwBXB6+se6AXhLhv3uAo6TtCitBnsxsGzQNsuAS9L5i4DbIyKAW4GTJLWmieOlwP0ZvrMsDu9o5slte0lCMzOrDVkSxFnA6ojYJunPgL8nGSsYVjqmcDnJH/sHgG9GxCpJV0u6IN3sS0CXpG7gfSSJiIjYCnyCJMncA9wdET8Y3amNn/kdLezu6WfH3r5KhWBmNuGyPEn9OeAUSacAfwN8Efgqyb/qhxURN5NcHipu+2DR/D7gdUPs+zWSW10r7vD25Anqx7ftpb21ocLRmJlNjCw9iL70ss+FwGci4lpgennDmlzmdSQPyz253eMQZlY7svQgdkq6kuT21pdIypGMQ9SMQg0mD1SbWS3J0oN4PbCf5HmIp0juRvpYWaOaZGa1NdFQJ57Y7ltdzax2ZCnW9xTw70C7pNcA+yLiq2WPbBLJ5cRh7c3uQZhZTclSauNPgd+QDCb/KXCnpIvKHdhk42chzKzWZBmD+DuSZyA2AkiaDfwXSe2kmjGvo4XfPLql0mGYmU2YLGMQuUJySG3OuF9VmdfRzFM79tGf98NyZlYbsvQgbpF0K/D1dPn1DHq2oRbM62ihPx9s3LnvwHMRZmbVbMQEERHvl/Ra4EVp03UR8d3yhjX5FG51Xb9lrxOEmdWELD0IIuLbwLfLHMuktrBrGgDrNu/mjEUzKxyNmVn5DZkgJO0ESl1wFxARMaNsUU1C8ztbqMuJdZv3VDoUM7MJMWSCiIiaKqcxkoa6HPM7Wli3xQnCzGpDzd2NdCiO6mpl3ebdlQ7DzGxCOEGMwlFdrax9xgnCzGqDE8QoLOyaxo59fWzb01PpUMzMym7YBCGpTtJPJiqYye7Ima0ArPVAtZnVgGETRET0A3lJ7RMUz6S2cNbBW13NzKpdlucgdgErJf0YOPCXMSLeU7aoJqlCD8K3uppZLciSIL6TTjWvuaGOw2Y0s9Y9CDOrAVlKbXxFUiNwfNq0OiJ6yxvW5LVo1jQe2eQEYWbVL8v7IF4GPAxcC3wWeEjS2WWOa9I6fm4b3U/vJHlNt5lZ9cpyienjwLkRsRpA0vEklV1/r5yBTVbHzZ3O7p5+nti+70ABPzOzapTlOYiGQnIAiIiHgIbyhTS5HT83qUDy0NM7KxyJmVl5ZUkQyyV9UdLL0ukLwPJyBzZZHTenDYCHnSDMrMplSRDvAu4H3pNO96dtI5J0nqTVkrolXVFifZOkb6Tr75S0cND6IyXtkvQ/snzfROic1sistiYeenpXpUMxMyurYccgJNUBX46INwKfGM2B032vBV4JbADukrQsIu4v2uxSYGtEHCvpYuAjJG+sK/gE8MPRfO9EOH5um3sQZlb1sjxJfVR6m+tonQF0R8SaiOgBbgAuHLTNhcBX0vkbgVdIEoCkPwIeBVaN4bvL6vi503l44y7yfj+1mVWxLHcxrQF+JWkZA5+kHqlHMR9YX7S8AThzqG0iok/SdqBL0j7gb0l6H0NeXpJ0GXAZwJFHHpnhVMbHcXPb2NPTz+Pb9nJE+nS1mVm1yTIG8Qjw/XTb6UVTOV0FXBMRw17oj4jrImJJRCyZPXt2mUM66ITDkpfpPfDkjgn7TjOziZZlDGJ6RIxlkPhx4Iii5QVpW6ltNkiqB9qBzSQ9jYskfRToICkYuC8iPjOGOMbd4sNnkBOsfHw75554WKXDMTMri2ETRET0S3rRGI99F3CcpEUkieBi4A2DtlkGXALcAVwE3B7JI8ovKWwg6Spg12RJDgAtjXUcP3c6Kx/fXulQzMzKJssYxD3p+MO3GDgGMWwBv3RM4XLgVqBwN9QqSVcDyyNiGfAl4HpJ3cAWkiQyJZw0v53bH9xIRJCOq5uZVZUsCaKZ5LLPOUVtQYYKrxFxM3DzoLYPFs3vA143wjGuyhDjhDt5QTvfWrHBJTfMrGplqeb6lokIZKo5aUEHACs3bHOCMLOqlKWa6/GSbpN0X7p8sqS/L39ok9sJh02nPifu3eBxCDOrTlluc/0CcCXQCxAR9zKFxgrKpbkhGaj+3YZtlQ7FzKwssiSI1oj4zaC2vnIEM9UsWdjJbx/bRm9/vtKhmJmNuywJ4hlJx5AMTCPpIuDJskY1RZy5qIs9Pf3c59tdzawKZUkQfwl8HjhB0uPAXwHvLGtUU8QZi2YCcOejWyociZnZ+BsxQaTF9n4fmA2cEBEvjoh15Q9t8ps9vYmjZ0/jN04QZlaFsvQgAIiI3SSvGrUiZy7q4q5Ht9Dvyq5mVmUyJ4jU/LJEMYWduWgmO/f3cf8TLtxnZtVltAnit2WJYgp74bFdAPzsoY0VjsTMbHyNKkFExFvLFchUNWd6MycvSOoymZlVkyFLbUhaSXpraykRcXJZIpqCXv6cOXz69ofZsruHmdPG8vI9M7PJZ7gexGuA84Fb0umN6fSsAny17pwT5hABP13tXoSZVY8hE0RErEtvZ31lRHwgIlam0xXAuRMX4uR30vx2ZrU1cZsvM5lZFckyBqHilwZJemHG/WpGLideuXgOP3lwI3t7+isdjpnZuMjyh/5S4LOS1kpaC3wW8GD1IOefMo89Pf381wNPVzoUM7NxkeV9ECuAUyS1p8suPFTCmYu6mDujiWW/e4LzT5lX6XDMzA5ZlvdBzJX0JeCGiNguabGkSycgtimlLifOP3keP129ke17eisdjpnZIctyienfSN4rXfhn8UMkBftskAtPnU9vf3DT7x6vdChmZocsS4KYFRHfBPIAEdEHeCS2hOfNn8FJ89u5/o51RLg2k5lNbVkSxG5JXRx8H8QLAI9DlCCJN511FA9v3MWv17jCq5lNbVkSxPuAZcAxkn4FfBV4d1mjmsIuOGUe7S0NfPWOtZUOxczskAx7F5OkHNAMvBR4DiBgdUR4FHYIzQ11LD3jSK77+SM8smkXx8xuq3RIZmZjMmwPIiLywLUR0RcRqyLiPieHkb3tJYtorM9x7U+6Kx2KmdmYZbnEdJuk10rSaA8u6TxJqyV1S7qixPomSd9I198paWHa/kpJKyStTD/PGe13V9KstibeeOZR3HTPE6zbvLvS4ZiZjUmWBPEO4FvAfkk7JO2UNOLbcSTVAdcCrwYWA0slLR602aXA1og4FrgG+Eja/gxwfkScBFwCXJ/pbCaRd5x9NI11OT5yy4OVDsXMbEyyvJN6ekTkIqIxImakyzMyHPsMoDt9p3UPcANw4aBtLgS+ks7fCLxCkiLitxHxRNq+CmiR1JTtlCaHOTOaeedLj+HmlU/x6zWbKx2OmdmoZSq6J6lT0hmSzi5MGXabD6wvWt7As19ZemCb9PmK7UDXoG1eC9wdEftLxHWZpOWSlm/atCnLqUyoy84+mnntzVy1bBW9/flKh2NmNipZSm28Dfg5ydPUH04/rypvWAe++0SSy07vKLU+Iq6LiCURsWT27NkTEdKotDTWcdUFJ/LgUzv5zO0esDazqSVLD+K9wOnAuoh4OXAasC3Dfo8DRxQtL0jbSm4jqR5oBzanywuA7wJvjohHMnzfpHTuiYfxx6fN59qfdHPf436+0MymjiwJYl9E7IPkrqOIeJDkmYiR3AUcJ2mRpEbgYpIH7ootIxmEBrgIuD0iQlIH8APgioj4VZYTmcyuOv9Eutoaec/Xf8uOfb5L2MymhiwJYkP6B/s/gR9LuglYN9JO6ZjC5SSXpB4AvhkRqyRdLemCdLMvAV2Sukme2C7cCns5cCzwQUn3pNOcUZ3ZJNLe2sA/L30+j23Zw1/dcA/5vOs0mdnkp9EUlZP0UpLLQLekdyZNGkuWLInly5dXOoxhXX/HWv7XTat4+0sW8T//4LmM4dESM7NxJWlFRCwptW7EFwZJOrJo8dH08zDgsXGIrab82QuSQn5f+MWjtLc0cPk5x1U6JDOzIY2YIEjGAoKkDlMzsAhYDZxYxriqkiSuOv9Edu7r459+9BCS+IuXHeOehJlNSlleOXpS8bKk5wN/UbaIqlwuJz520cnkI/jYravZtHM/H3zNYnI5Jwkzm1yy9CAGiIi7JZ1ZjmBqRX1djmv+9FRmtzXxxV8+yrrNu7nm9afS0dpY6dDMzA7IMgbxvqLFHPB84IkhNreMcjnx969ZzFFdrVz9/fv5w0//kk8vPY3fO6qz0qGZmQHZbnOdXjQ1kYxJDK6pZGP0prMW8q13vhCAi/7lv/nw91axe39fhaMyMxvlba6T2VS4zXU4O/f18tFbVnP9r9cxv6OF97/qOVxwyjyPTZhZWQ13m+uICULS90jfR11KRFww1LqJNNUTRMFda7dw1bJVrHpiB889fAbvf9XxvOz4OU4UZlYWh/QcBLCG5LmHr6XLS4GnSZ6stnF2+sKZfO/yF/O9e5/gY7eu5q3/tpxj57Txthcv4o9Om09zQ12lQzSzGpGlB7F8cHYp1VZp1dKDKNbTl+cHK5/gCz9/lPuf3EFHawPnnzyP1/7eAk5Z0O7nJ8zskB1qD2KapKMjYk16sEXAtPEM0EprrM/xx6ct4I9Onc8dazbz9d+s55vL13P9r9dx9OxpnHfiYbxy8VxOWdDhS1BmNu6y9CDOA64judQk4Cjgsoj4UfnDy64aexCl7NjXy833PslN9zzBb9ZuoT8fzGpr4uXPmc1Zx3TxgqO7mNfRUukwzWyKOKRB6vQATcAJ6eKDpd7uVmm1kiCKbd/Ty08f2siP7n+aX3U/w7Y9SSnxI2e2csaimZxyRAcnzW/nhMOme+zCzEoaU4KQdDqwPiKeSpffTPL6z3XAVRGxpUzxjkktJohi+Xyw+umd/HrNZn69ZjN3rd3Klt1Jwd36nHjOYdM5cd4MjpsznWPntnHs7Dbmd7T40pRZjRtrgrgb+P2I2JK+g/oG4N3AqcBzI+KicgU8FrWeIAaLCJ7Yvo+VG7Zx74btrHx8O/c/sYPNuw9WaW9pqOOYOdNYNKuNBZ0tHNHZmnzObGVeRzNN9e51mFW7sQ5S1xX1El4PXBcR3wa+Leme8Q7Sxpck5ne0ML+jhfOed/iB9q27e+jetIuHn95F98ZddG/axb0btvHDlU/SV/QiIwnmTm9mbnszc6Y3MXdGE3OmJ/NzCvMzmuia1kSdeyFmVWnYBCGpPn0z3CuAyzLuZ5NY57RGTp82k9MXzhzQ3p8Pnt6xj/Vb9rB+6142bN3D+i172bhzH49t3sPytVvYuqf061JnNNfTOa2RjtZGOlsbmNl6cL5jWvI5o7mBtuZ6pjfVJ5/NDbQ21PkSl9kkNtwf+q8DP5P0DLAX+AWApGOB7RMQm02gupyY19HCvI4WhirVu7+vn00797Nx53427tjPpp372Ly7h627e9i6p5ete3rYvKuH7o272Lanl10j1JSSoK2xkDDqaWuqp625gelN9bQ21tFSmBrqkuWGOpob6mhtrKelMUdLQ/2A9c0NB7d3r8bs0A2ZICLiHyTdBhwO/CgODlbkSMYirMY01dexoLOVBZ2tmbbv6cuzbU+SPHbt72XHvj527etj1/4+du7rZde+PnbuT9p2pu3b91oj7kwAAAk/SURBVPayYese9vb0s6enn729/fT05Ucda11ONNblaGrI0ViXo7E+R1N9jsb6ugPzTfW5Etsk6w9un6Mhl6O+TtTX5WjIpZ91oj5tHzifoz6XfqbtDYP2ra/TwWPm5AcebdIa9lJRRPy6RNtD5QvHqkljfY45M5qZM6P5kI7Tnw/29vazt6effb0HE8eenj729faztyd/cD5d7ulPEsv+vjw96bS/P8/+3jw9/Xl6+vrZvb+PLYX1A9YdbJsI9TlRXyfqJHK5JGnU5UROyXwuXa5T+pmuK8wX1uVyUJ/LJduLAesHbJ/OF76r5LGUjGPl0vlcTkik65L5gevS+VL7Dti+eH3alhvl9sXrcwO3K16vdD9x8JgCKHwHA9dT2GfA9kXHQs9eX+XJ3WMJNunV5ZRcfmqa2P9d8/mgpz9PXz7o68/T2x/05fP09aft/UHvEOsL7b3pdn35dP2B9nSf/MG2/uIpgvyg5f58kI+grz/5TNqhP59P1uVhb38/ffmD++YjDi7HwO8oHKN4fT4Pffk8AVRJoecJkStKRho8jwasRwMTViH5USoBpeuLk1SpYz338Bn889LTxv28nCDMhpDLieZc7d7qGxFEkCSSovl8+hn5wnLSFkXr8qW2L16fZ9A2ozhGfmDbwH2T9f0REBAkxwgOfl8kJ3dgn0IyzKcZMdL9kvVFxyjav7B94ViDjx8Djl04j4PHYsD2B9dz4LsOHv9Zx4IBsRNwRGd5qic4QZhZSYV/ueaQ/1DUqCxvlDMzsxpU1gQh6TxJqyV1S7qixPomSd9I198paWHRuivT9tWSXlXOOM3M7NnKliAk1QHXAq8GFgNLJS0etNmlwNaIOBa4BvhIuu9i4GLgROA84LPp8czMbIKUswdxBtAdEWsiooekltOFg7a5EPhKOn8j8Aol941dCNwQEfsj4lGgOz2emZlNkHImiPnA+qLlDWlbyW3Skh7bga6M+yLpMknLJS3ftGnTOIZuZmZTepA6Iq6LiCURsWT27NmVDsfMrKqUM0E8DhxRtLwgbSu5jaR6oB3YnHFfMzMro3ImiLuA4yQtktRIMui8bNA2y4BL0vmLgNvTmk/LgIvTu5wWAccBvyljrGZmNkjZnn+JiD5JlwO3AnXAlyNilaSrgeURsQz4EnC9pG5gC0kSId3um8D9QB/wlxHRP9z3rVix4hlJ6w4h5FnAM4ew/1Tkc65+tXa+4HMeraOGWpHpndS1QNLyod6qVK18ztWv1s4XfM7jaUoPUpuZWfk4QZiZWUlOEAddV+kAKsDnXP1q7XzB5zxuPAZhZmYluQdhZmYlOUGYmVlJNZ8gRipJPlVJOkLSTyTdL2mVpPem7TMl/VjSw+lnZ9ouSZ9O/zvcK+n5lT2DsZNUJ+m3kr6fLi9Ky8l3p+XlG9P2IcvNTyWSOiTdKOlBSQ9IOqvaf2dJf53+f32fpK9Laq6231nSlyVtlHRfUduof1dJl6TbPyzpklLfNZSaThAZS5JPVX3A30TEYuAFwF+m53YFcFtEHAfcli5D8t/guHS6DPjcxIc8bt4LPFC0/BHgmrSs/FaSMvMwRLn5KehTwC0RcQJwCsm5V+3vLGk+8B5gSUQ8j+RB3Iupvt/530hed1BsVL+rpJnAh4AzSSpif6iQVDKJ9D2vtTgBZwG3Fi1fCVxZ6bjKdK43Aa8EVgOHp22HA6vT+c8DS4u2P7DdVJpI6nbdBpwDfJ/kve/PAPWDf3OSp/zPSufr0+1U6XMY5fm2A48Ojruaf2cOVnuemf5u3wdeVY2/M7AQuG+svyuwFPh8UfuA7UaaaroHQcay4lNd2qU+DbgTmBsRT6arngLmpvPV8t/ik8AHgHy63AVsi6ScPAw8r6HKzU8li4BNwL+ml9W+KGkaVfw7R8TjwD8BjwFPkvxuK6ju37lgtL/rIf3etZ4gqp6kNuDbwF9FxI7idZH8k6Jq7nOW9BpgY0SsqHQsE6geeD7wuYg4DdjNwcsOQFX+zp0kLxVbBMwDpvHsSzFVbyJ+11pPEFVdVlxSA0ly+PeI+E7a/LSkw9P1hwMb0/Zq+G/xIuACSWtJ3mB4Dsn1+Y60nDwMPK+hys1PJRuADRFxZ7p8I0nCqObf+feBRyNiU0T0At8h+e2r+XcuGO3veki/d60niCwlyackSSKplvtARHyiaFVxifVLSMYmCu1vTu+GeAGwvagrOyVExJURsSAiFpL8lrdHxBuBn5CUk4dnn3OpcvNTRkQ8BayX9Jy06RUkVZCr9ncmubT0Akmt6f/nhXOu2t+5yGh/11uBcyV1pj2vc9O2bCo9CFPpCfgD4CHgEeDvKh3POJ7Xi0m6n/cC96TTH5Bce70NeBj4L2Bmur1I7uh6BFhJcodIxc/jEM7/ZcD30/mjSd4n0g18C2hK25vT5e50/dGVjnuM53oqsDz9rf8T6Kz23xn4MPAgcB9wPdBUbb8z8HWSMZZekp7ipWP5XYG3pufeDbxlNDG41IaZmZVU65eYzMxsCE4QZmZWkhOEmZmV5ARhZmYlOUGYmVlJThBmI5DUL+meomncqv5KWlhcrdNsMqkfeROzmrc3Ik6tdBBmE809CLMxkrRW0kclrZT0G0nHpu0LJd2e1uW/TdKRaftcSd+V9Lt0emF6qDpJX0jfb/AjSS3p9u9R8j6PeyXdUKHTtBrmBGE2spZBl5heX7Rue0ScBHyGpJIswD8DX4mIk4F/Bz6dtn8a+FlEnEJSL2lV2n4ccG1EnAhsA16btl8BnJYe553lOjmzofhJarMRSNoVEW0l2tcC50TEmrQw4lMR0SXpGZKa/b1p+5MRMUvSJmBBROwvOsZC4MeRvAAGSX8LNETE/5F0C7CLpHzGf0bErjKfqtkA7kGYHZoYYn409hfN93NwbPAPSerrPB+4q6hSqdmEcIIwOzSvL/q8I53/b5JqsgBvBH6Rzt8GvAsOvDe7faiDSsoBR0TET4C/JSlR/axejFk5+V8kZiNrkXRP0fItEVG41bVT0r0kvYCladu7Sd7w9n6St729JW1/L3CdpEtJegrvIqnWWUod8LU0iQj4dERsG7czMsvAYxBmY5SOQSyJiGcqHYtZOfgSk5mZleQehJmZleQehJmZleQEYWZmJTlBmJlZSU4QZmZWkhOEmZmV9P8BhJjA615gYeYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_2d():\n",
    "    #D_in is input dimension\n",
    "    #H1 is dimension of first hidden layer \n",
    "    #H2 is dimension of second hidden layer\n",
    "    #D_out is output dimension.\n",
    "    epochs = 1000\n",
    "    learning_rate = 0.001\n",
    "    inputsize = 3\n",
    "    loss_function = 'squared'\n",
    "    \n",
    "    onehotencoded = False\n",
    "    minibatch = False\n",
    "    train_X, train_Y = generateData(inputsize, onehotencoded, minibatch)\n",
    "    train_Y = train_Y.reshape((train_Y.shape[0], 1))\n",
    "    \n",
    "    print(train_X.shape, train_Y.shape)\n",
    "\n",
    "    D_in, H1, H2, H3, H4, D_out = inputsize, 10, 10, 10, 10, 1 \n",
    "    # list of number of neurons in the layers sequentially.\n",
    "    neurons = [D_in, H1, H2, H3, H4, D_out] \n",
    "    # activations in each layer (Note: the input layer does not have any activation)\n",
    "    activation_functions = ['linear','linear', 'tanh', 'relu', 'sigmoid'] \n",
    "\n",
    "    # Train the network\n",
    "    neuralnet = Neural_Network(neurons, activation_functions, epochs, learning_rate, loss_function)\n",
    "    loss = neuralnet.train(train_X, train_Y, minibatch=minibatch)\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.plot(loss)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Squared-error loss without mini-batch')\n",
    "    plt.show()\n",
    "\n",
    "plot_2d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2(1)(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 20, 3) (25, 20, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_2e():\n",
    "    #D_in is input dimension\n",
    "    #H1 is dimension of first hidden layer \n",
    "    #H2 is dimension of second hidden layer\n",
    "    #D_out is output dimension.\n",
    "    epochs = 1000\n",
    "    learning_rate = 10 ** -3\n",
    "    inputsize = 3\n",
    "    loss_function = 'squared'\n",
    "\n",
    "    onehotencoded = False\n",
    "    minibatch = True\n",
    "    batch_size = 20\n",
    "    train_X, train_Y = generateData(inputsize, onehotencoded, minibatch, batch_size)\n",
    "    num_batches = train_X.shape[0]\n",
    "    train_Y = train_Y.reshape(num_batches, batch_size, 1)\n",
    "    \n",
    "    print(train_X.shape, train_Y.shape)\n",
    "\n",
    "    D_in, H1, H2, H3, H4, D_out = inputsize, 10, 10, 10, 10, 1 \n",
    "    # list of number of neurons in the layers sequentially.\n",
    "    neurons = [D_in, H1, H2, H3, H4, D_out] \n",
    "    # activations in each layer (Note: the input layer does not have any activation)\n",
    "    activation_functions = ['linear','linear', 'tanh', 'relu', 'sigmoid'] \n",
    "\n",
    "    # Train the network\n",
    "    neuralnet = Neural_Network(neurons, activation_functions, epochs, learning_rate, loss_function)\n",
    "    loss = neuralnet.train(train_X, train_Y, minibatch=minibatch)\n",
    "\n",
    "    # Plot the results\n",
    "    plt.plot(loss)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Squared-error loss with mini-batch size = 20')\n",
    "    plt.show()\n",
    "\n",
    "plot_2e()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Question 2(1)(f)\n",
    "### Observation \n",
    "Stochastic gradient descent performs better than mini-batch gradient descent.\n",
    "### Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Question 2(2)\n",
    "\n",
    "### Exploding and vanishing gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loadData_3():\n",
    "    data = datasets.load_digits()\n",
    "    return data['data'], data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_3():\n",
    "    X, Y = loadData_3()\n",
    "    X = X / 255\n",
    "    return X, Y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [],
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Neural network with Squared-Error loss\n",
    "def train_3_se(batch_size=1, learning_rate=0.0001): \n",
    "    # Number of neurons in each layer\n",
    "    neurons = [64, 256, 128, 1]\n",
    "    activations = ['sigmoid', 'sigmoid', 'relu']\n",
    "    loss_function = 'squared'\n",
    "    epochs = 200\n",
    "\n",
    "    # Data preprocessing\n",
    "    onehotencoded = False\n",
    "    minibatch = True\n",
    "    X, Y = preprocess_3()\n",
    "    # Need to choose train set size based on the batch_sizes in question 3(e)\n",
    "    X, Y = X[:1500], Y[:1500]\n",
    "    X, Y = create_minibatches(X, Y, batch_size)\n",
    "    \n",
    "    # Split training and test data\n",
    "    num_training = 1500\n",
    "    num_batches = num_training // batch_size\n",
    "    train_X, train_Y = X[: num_batches], Y[: num_batches]    \n",
    "    train_Y = train_Y.reshape((num_batches, batch_size, 1))\n",
    "\n",
    "    print(train_X.shape, train_Y.shape)\n",
    "    \n",
    "    # Train the network\n",
    "    neuralnet = Neural_Network(neurons, activations, epochs, learning_rate, loss_function)\n",
    "    loss = neuralnet.train(train_X, train_Y, minibatch=minibatch)    \n",
    "    return neuralnet, loss\n",
    "    \n",
    "# neuralnet, loss = train_3_se([1], [0.0001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network with Cross-Entropy loss\n",
    "def train_3_ce(batch_size=1, learning_rate=0.0001):\n",
    "    # Number of neurons in each layer\n",
    "    neurons = [64, 256, 128, 10]\n",
    "    activations = ['sigmoid', 'sigmoid', 'softmax']\n",
    "    loss_function = 'cross-entropy'\n",
    "    epochs = 200\n",
    "\n",
    "    # Data preprocessing\n",
    "    onehotencoded = False\n",
    "    minibatch = True\n",
    "    X, Y = preprocess_3()\n",
    "    # Need to choose train set size based on the batch_sizes in question 3(e)\n",
    "    X, Y = X[:1500], Y[:1500]\n",
    "    Y = onehotencoding(Y)\n",
    "    X, Y = create_minibatches(X, Y, batch_size)\n",
    "\n",
    "    # Split training and test data\n",
    "    num_training = 1500\n",
    "    num_batches = num_training // batch_size\n",
    "    train_X, train_Y = X[: num_batches], Y[: num_batches]\n",
    "    train_Y = train_Y.reshape((num_batches, batch_size, 10))    \n",
    "\n",
    "    print(train_X.shape, train_Y.shape)\n",
    "    \n",
    "    # Train the network\n",
    "    neuralnet = Neural_Network(neurons, activations, epochs, learning_rate, loss_function)\n",
    "    loss = neuralnet.train(train_X, train_Y, minibatch=minibatch)\n",
    "    return neuralnet, loss\n",
    "\n",
    "# neuralnet, loss = train_3_ce()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Question 3(a)\n",
    "\n",
    "### Squared Error Loss\n",
    "\n",
    "Squared-error loss function is used in the regression scenario with number of output layer neurons = 1.\n",
    "This neuron predicts values in the range 0 to 9. The activation function used here is ReLU.\n",
    "\n",
    "### Cross-Entropy Loss\n",
    "CE loss function is used for multi-class classification and the number of output layer neurons in this case = 10.\n",
    "The output layer returns one-hot encoded values for labels in the range 0 to 9.\n",
    "Activation function used in this case is softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3(b)\n",
    "### Plot squared-error and cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 1, 64) (1500, 1, 1)\n",
      "(1500, 1, 64) (1500, 1, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_3b():\n",
    "    neuralnet_se, loss_se = train_3_se()\n",
    "    neuralnet_ce, loss_ce = train_3_ce()\n",
    "    plt.ylabel(f'Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    \n",
    "    line, = plt.plot(loss_se, 'b')\n",
    "    line.set_label(f'Squared-error loss')\n",
    "    line, = plt.plot(loss_ce, 'r')\n",
    "    line.set_label(f'Cross-entropy loss')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_3b()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3(c)\n",
    "### Comparision of squared-error loss by varying learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 1, 64) (1500, 1, 1)\n",
      "(1500, 1, 64) (1500, 1, 1)\n",
      "(1500, 1, 64) (1500, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "def plot_3c():\n",
    "    batch_size = 1\n",
    "    lrs = [10 ** -1, 10 ** -2, 10 ** -3, 10 ** -5, 10 ** -6]\n",
    "    losses = []\n",
    "    for lr in lrs:\n",
    "        neuralnet, loss = train_3_se(batch_size, lr)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    # Plot the losses\n",
    "    colors = ['c', 'r', 'y', 'k', 'g']\n",
    "    for i in range(len(losses)):\n",
    "        line, = plt.plot(losses[i], colors[i])\n",
    "        line.set_label(f'Learning rate = {lrs[i]}')\n",
    "    \n",
    "    plt.ylabel('Squared-Error loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_3c()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3(d)\n",
    "### Comparision of cross-entropy loss by varying learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d():\n",
    "    batch_size = 1\n",
    "    lrs = [10 ** -1, 10 ** -2, 10 ** -3, 10 ** -5, 10 ** -6]\n",
    "    losses = []\n",
    "    for lr in lrs:\n",
    "        neuralnet, loss = train_3_ce(batch_size, lr)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    # Plot the losses\n",
    "    colors = ['c', 'r', 'y', 'k', 'g']\n",
    "    for i in range(len(losses)):\n",
    "        line, = plt.plot(losses[i], colors[i])\n",
    "        line.set_label(f'Learning rate = {lrs[i]}')\n",
    "    \n",
    "    plt.ylabel('Cross-entropy loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_3d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparision of squared-error loss by varying mini-batch size\n",
    "\n",
    "Learning rate = 10^(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3e_se():\n",
    "    lr = 10 ** -5\n",
    "    batch_sizes = [50, 100, 250, 300, 500]\n",
    "    losses = []\n",
    "    for batch_size in batch_sizes:\n",
    "        neuralnet, loss = train_3_se(batch_size, lr)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    # Plot the losses\n",
    "    colors = ['c', 'r', 'y', 'k', 'g']\n",
    "    for i in range(len(losses)):\n",
    "        line, = plt.plot(losses[i], colors[i])\n",
    "        line.set_label(f'Batch size = {batch_sizes[i]}')\n",
    "    \n",
    "    plt.ylabel('Squared-error loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_3e_se()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparision of cross-entropyloss by varying mini-batch size\n",
    "\n",
    "Learning rate = 10^(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3e_ce():\n",
    "    lr = 10 ** -5\n",
    "    batch_sizes = [50, 100, 250, 300, 500]\n",
    "    losses = []\n",
    "    for batch_size in batch_sizes:\n",
    "        neuralnet, loss = train_3_ce(batch_size, lr)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    # Plot the losses\n",
    "    colors = ['c', 'r', 'y', 'k', 'g']\n",
    "    for i in range(len(losses)):\n",
    "        line, = plt.plot(losses[i], colors[i])\n",
    "        line.set_label(f'Batch size = {batch_sizes[i]}')\n",
    "    \n",
    "    plt.ylabel('Squared-error loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_3e_ce()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Observation\n",
    "In both the cases, with increasing batch-size, performance degrades.\n",
    "\n",
    "### Reason\n",
    "The number of updates is inversely proportional to the batch-size.\n",
    "As the batch size increases, fewer corrections are made to the weights.\n",
    "This leads to underfitting and error decreases slowly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Question 4(a)\n",
    "### Split the data into S1 and S2 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def split_data_4a():\n",
    "    x, y = preprocess_3()\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(x)\n",
    "    np.random.shuffle(y)\n",
    "    num_samples = x.shape[0]\n",
    "    num_samples_s1 = int((num_samples * 0.8 // 50) * 50)\n",
    "    num_samples_s2 = int((num_samples * 0.2 // 50) * 50)\n",
    "    s1 = (x[:num_samples_s1], y[:num_samples_s1])\n",
    "    s2 = (x[:num_samples_s2], y[:num_samples_s2])\n",
    "    return (s1, s2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4(b)\n",
    "### Functions to compute squared-error and cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network with Squared-Error loss\n",
    "def train_4b_se(learning_rate=0.0001): \n",
    "    # Number of neurons in each layer\n",
    "    neurons = [64, 256, 128, 1]\n",
    "    activations = ['sigmoid', 'sigmoid', 'relu']\n",
    "    loss_function = 'squared'\n",
    "    epochs = 200\n",
    "\n",
    "    # Data preprocessing\n",
    "    onehotencoded = False\n",
    "    minibatch = True\n",
    "    batch_size = 50\n",
    "    samples = split_data_4a()\n",
    "\n",
    "    neuralnets = []\n",
    "    losses = []\n",
    "    for sample in samples:\n",
    "        X, Y = sample\n",
    "\n",
    "        # Create mini-batches\n",
    "        num_batches = int(X.shape[0] / batch_size)\n",
    "        train_X, train_Y = create_minibatches(X, Y, batch_size)\n",
    "        train_Y = train_Y.reshape((num_batches, batch_size, 1))\n",
    "\n",
    "        print(train_X.shape, train_Y.shape)\n",
    "\n",
    "         # Train the network\n",
    "        neuralnet = Neural_Network(neurons, activations, epochs, learning_rate, loss_function)\n",
    "        loss = neuralnet.train(train_X, train_Y, minibatch=minibatch)    \n",
    "        \n",
    "        neuralnets.append(neuralnet)\n",
    "        losses.append(loss)\n",
    "        \n",
    "    return neuralnets, losses\n",
    "\n",
    "# neuralnet, loss = train_4b_se()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network with Cross-Entropy loss\n",
    "def train_4b_ce(learning_rate=0.0001): \n",
    "    # Number of neurons in each layer\n",
    "    neurons = [64, 256, 128, 10]\n",
    "    activations = ['sigmoid', 'sigmoid', 'softmax']\n",
    "    loss_function = 'cross-entropy'\n",
    "    epochs = 200\n",
    "\n",
    "    # Data preprocessing\n",
    "    onehotencoded = False\n",
    "    minibatch = True\n",
    "    batch_size = 50\n",
    "    samples = split_data_4a()\n",
    "    \n",
    "    neuralnets = []\n",
    "    losses = []\n",
    "    for sample in samples:\n",
    "        X, Y = sample\n",
    "\n",
    "        # Create mini-batches\n",
    "        num_batches = int(X.shape[0] / batch_size)\n",
    "        train_X, train_Y = create_minibatches(X, Y, batch_size)\n",
    "        train_Y = train_Y.reshape((num_batches, batch_size, 1))\n",
    "\n",
    "        print(train_X.shape, train_Y.shape)\n",
    "\n",
    "         # Train the network\n",
    "        neuralnet = Neural_Network(neurons, activations, epochs, learning_rate, loss_function)\n",
    "        loss = neuralnet.train(train_X, train_Y, minibatch=minibatch)        \n",
    "        \n",
    "        neuralnets.append(neuralnet)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    return neuralnets, losses\n",
    "\n",
    "# neuralnet, loss = train_4b_ce()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4(c)\n",
    "### Selecting a suitable learning rate for squared-error loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_4c():\n",
    "    lrs = [10**-1, 10**-2, 10**-3, 10**-4, 10**-5, 10**-6]\n",
    "    losses_s1, losses_s2 = [], []\n",
    "    \n",
    "    for lr in lrs:\n",
    "        neuralnets, losses = train_4b_se(lr)\n",
    "        # Record every 5th epoch loss\n",
    "        losses_s1.append(losses[0][::5])\n",
    "        losses_s2.append(losses[1][::5])\n",
    "    \n",
    "    # Plot the losses for the two samples\n",
    "    colors = ['c', 'r', 'y', 'k', 'g', 'y']\n",
    "    for i in range(len(losses_s1)):\n",
    "        line, = plt.plot(losses_s1[i], colors[i])\n",
    "        line.set_label(f'Learning rate = {lrs[i]}')\n",
    "    plt.ylabel('Squared-error loss for S1')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    for i in range(len(losses_s2)):\n",
    "        line, = plt.plot(losses_s2[i], colors[i])\n",
    "        line.set_label(f'Learning rate = {lrs[i]}')\n",
    "    plt.ylabel('Squared-error loss for S2')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_4c()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4(d)\n",
    "### Selecting a suitable learning rate for cross-entropy loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_4c():\n",
    "    lrs = [10**-1, 10**-2, 10**-3, 10**-4, 10**-5, 10**-6]\n",
    "    losses_s1, losses_s2 = [], []\n",
    "    \n",
    "    for lr in lrs:\n",
    "        neuralnets, losses = train_4b_ce(lr)\n",
    "        # Record every 5th epoch loss\n",
    "        losses_s1.append(losses[0][::5])\n",
    "        losses_s2.append(losses[1][::5])\n",
    "    \n",
    "    # Plot the losses for the two samples\n",
    "    colors = ['c', 'r', 'y', 'k', 'g', 'y']\n",
    "    for i in range(len(losses_s1)):\n",
    "        line, = plt.plot(losses_s1[i], colors[i])\n",
    "        line.set_label(f'Learning rate = {lrs[i]}')\n",
    "    plt.ylabel('Squared-error loss for S1')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    for i in range(len(losses_s2)):\n",
    "        line, = plt.plot(losses_s2[i], colors[i])\n",
    "        line.set_label(f'Learning rate = {lrs[i]}')\n",
    "    plt.ylabel('Squared-error loss for S2')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_4c()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4(e)\n",
    "### Comparision between learning rates for 3(c), 3(d) with 4(c), 4(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
